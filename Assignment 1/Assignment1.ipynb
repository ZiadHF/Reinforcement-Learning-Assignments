{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50fa7463",
   "metadata": {},
   "source": [
    "# Policy Iteration Visualization with Pygame\n",
    "\n",
    "This notebook demonstrates how **Policy Iteration** progressively improves the policy in a grid maze environment. We'll visualize:\n",
    "- How the policy changes in each iteration\n",
    "- How the value function evolves\n",
    "- The convergence process toward the optimal policy\n",
    "\n",
    "Each iteration will be displayed in real-time using Pygame, showing the transformation from a random initial policy to the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db983691",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for environment creation, dynamic programming algorithms, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "293d26a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.1)\n",
      "Requirement already satisfied: pygame in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.10.7)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abdul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "âœ“ All libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\abdul\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium pygame numpy matplotlib\n",
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import sys\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a8292",
   "metadata": {},
   "source": [
    "## 2. Grid Maze Environment With Proper Visualisation\n",
    "\n",
    "We extend the original GridMazeEnv to support policy rendering with arrows, value function heatmaps, and iteration tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "c64dbbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridMazeEnv created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: ENHANCED GRID MAZE ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "class GridMazeEnvWithVisualization(gym.Env):\n",
    "    \"\"\"\n",
    "    Enhanced GridMaze environment with built-in policy and value visualization.\n",
    "    \n",
    "    Additions to original environment:\n",
    "    - Policy rendering (arrows showing action directions)\n",
    "    - Value function heatmap\n",
    "    - Iteration counter display\n",
    "    - Highlighting of changed states\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    \n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size\n",
    "        self.window_size = 800  \n",
    "        \n",
    "        # Action and observation spaces\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        low = np.zeros(8, dtype=int)  # Agent(2) + Goal(2) + Bad1(2) + Bad2(2)\n",
    "        high = np.full(8, self.size - 1, dtype=int)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, dtype=int)\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "        # Visualization attributes\n",
    "        self.font = None\n",
    "        self.small_font = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate([\n",
    "            self._agent_location,\n",
    "            self._goal_location,\n",
    "            self._bad_cells[0],\n",
    "            self._bad_cells[1]\n",
    "        ])\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        all_cells = np.array(range(self.size * self.size))\n",
    "        selected_indices = self.np_random.choice(all_cells, size=4, replace=False)\n",
    "        \n",
    "        def index_to_coords(index):\n",
    "            return np.array([index % self.size, index // self.size])\n",
    "\n",
    "        self._agent_location = index_to_coords(selected_indices[0])\n",
    "        self._goal_location = index_to_coords(selected_indices[1])\n",
    "        self._bad_cells = [\n",
    "            index_to_coords(selected_indices[2]),\n",
    "            index_to_coords(selected_indices[3])\n",
    "        ]\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Stochastic transitions (same as before)\n",
    "        outcome_actions = [\n",
    "            (action - 1) % 4,\n",
    "            action,\n",
    "            (action + 1) % 4\n",
    "        ]\n",
    "        \n",
    "        effective_move_index = self.np_random.choice([0, 1, 2], p=[0.15, 0.70, 0.15])\n",
    "        effective_action = outcome_actions[effective_move_index]\n",
    "\n",
    "        movement = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, -1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, 1])\n",
    "        }[effective_action]\n",
    "        \n",
    "        new_location = np.clip(\n",
    "            self._agent_location + movement, 0, self.size - 1\n",
    "        )\n",
    "        self._agent_location = new_location\n",
    "\n",
    "        terminated = False\n",
    "        reward = -1.0\n",
    "\n",
    "        if np.array_equal(self._agent_location, self._goal_location):\n",
    "            reward = 100.0\n",
    "            terminated = True\n",
    "        \n",
    "        for bad_cell in self._bad_cells:\n",
    "            if np.array_equal(self._agent_location, bad_cell):\n",
    "                reward = -100.0\n",
    "                terminated = True\n",
    "                break\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    def _render_frame(self):\n",
    "        # Initialize pygame if not already done (needed for both human and rgb_array modes)\n",
    "        if self.font is None:\n",
    "            pygame.init()\n",
    "            self.font = pygame.font.Font(None, 36)\n",
    "            self.small_font = pygame.font.Font(None, 24)\n",
    "        \n",
    "        # Initialize window only for human mode\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "            pygame.display.set_caption(\"Grid Maze - Policy Iteration Visualization\")\n",
    "        \n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_sq_size = self.window_size / self.size\n",
    "\n",
    "        # Draw grid\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas, (200, 200, 200), \n",
    "                (0, pix_sq_size * x), (self.window_size, pix_sq_size * x), width=3\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas, (200, 200, 200), \n",
    "                (pix_sq_size * x, 0), (pix_sq_size * x, self.window_size), width=3\n",
    "            )\n",
    "\n",
    "        # Draw goal\n",
    "        pygame.draw.rect(\n",
    "            canvas, (0, 255, 0), pygame.Rect(\n",
    "                pix_sq_size * self._goal_location[0],\n",
    "                pix_sq_size * self._goal_location[1],\n",
    "                pix_sq_size, pix_sq_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Draw bad cells\n",
    "        for bad_cell in self._bad_cells:\n",
    "             pygame.draw.rect(\n",
    "                canvas, (255, 0, 0), pygame.Rect(\n",
    "                    pix_sq_size * bad_cell[0],\n",
    "                    pix_sq_size * bad_cell[1],\n",
    "                    pix_sq_size, pix_sq_size,\n",
    "                ),\n",
    "            )\n",
    "        \n",
    "        # Draw agent\n",
    "        pygame.draw.circle(\n",
    "            canvas, (0, 0, 255), (\n",
    "                (self._agent_location[0] + 0.5) * pix_sq_size,\n",
    "                (self._agent_location[1] + 0.5) * pix_sq_size,\n",
    "            ), pix_sq_size / 3,\n",
    "        )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        \n",
    "        return canvas\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Return the rendered frame as RGB array or display to screen.\"\"\"\n",
    "        canvas = self._render_frame()\n",
    "        \n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            # Convert pygame surface to numpy array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        # For human mode, rendering is already done in _render_frame\n",
    "        return None\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "print(\"GridMazeEnv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4115a4",
   "metadata": {},
   "source": [
    "## 3. Policy Visualization Renderer\n",
    "\n",
    "Create a specialized class for rendering policies with arrows, value function heatmaps, and change highlighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "2f8328dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyVisualizer created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: POLICY VISUALIZATION RENDERER\n",
    "# ============================================================================\n",
    "\n",
    "class PolicyVisualizer:\n",
    "    \"\"\"\n",
    "    Renders policy and value function with rich visual feedback.\n",
    "    \n",
    "    Features:\n",
    "    - Arrow rendering for policy directions\n",
    "    - Value function heatmap\n",
    "    - Highlighting of changed states\n",
    "    - Iteration counter and statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, window_size=800):\n",
    "        self.env = env\n",
    "        self.window_size = window_size\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.font = None\n",
    "        self.small_font = None\n",
    "        self.tiny_font = None\n",
    "        \n",
    "        # Color maps for value function\n",
    "        self.colormap = cm.get_cmap('RdYlGn')  # Red (low) -> Yellow -> Green (high)\n",
    "        \n",
    "    def initialize_pygame(self):\n",
    "        \"\"\"Initialize pygame window and fonts.\"\"\"\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size + 100))\n",
    "            pygame.display.set_caption(\"Policy Iteration - Real-time Visualization\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "            self.font = pygame.font.Font(None, 48)\n",
    "            self.small_font = pygame.font.Font(None, 32)\n",
    "            self.tiny_font = pygame.font.Font(None, 20)\n",
    "    \n",
    "    def render_policy_and_values(self, policy, V, iteration=0, changed_states=None, \n",
    "                                  title=\"Policy Iteration\", delta=None):\n",
    "        \"\"\"\n",
    "        Render the current policy with arrows and value function as heatmap.\n",
    "        \n",
    "        Args:\n",
    "            policy: Current policy (action for each state)\n",
    "            V: Current value function\n",
    "            iteration: Current iteration number\n",
    "            changed_states: Set of state indices that changed\n",
    "            title: Title to display\n",
    "            delta: Convergence delta value\n",
    "        \"\"\"\n",
    "        self.initialize_pygame()\n",
    "        \n",
    "        canvas = pygame.Surface((self.window_size, self.window_size + 100))\n",
    "        canvas.fill((240, 240, 240))\n",
    "        \n",
    "        pix_sq_size = self.window_size / self.env.size\n",
    "        \n",
    "        # Normalize values for color mapping\n",
    "        V_min, V_max = V.min(), V.max()\n",
    "        if V_max - V_min > 0:\n",
    "            V_normalized = (V - V_min) / (V_max - V_min)\n",
    "        else:\n",
    "            V_normalized = np.zeros_like(V)\n",
    "        \n",
    "        # Draw each cell\n",
    "        for state_idx in range(self.env.size * self.env.size):\n",
    "            x, y = state_idx % self.env.size, state_idx // self.env.size\n",
    "            state_coords = np.array([x, y])\n",
    "            \n",
    "            # Determine cell color based on value function\n",
    "            color_val = V_normalized[state_idx]\n",
    "            rgb = self.colormap(color_val)[:3]  # Get RGB, ignore alpha\n",
    "            cell_color = tuple(int(c * 255) for c in rgb)\n",
    "            \n",
    "            # Highlight changed states\n",
    "            if changed_states and state_idx in changed_states:\n",
    "                cell_color = (255, 255, 0)  # Yellow for changed\n",
    "            \n",
    "            # Check if terminal state\n",
    "            is_goal = np.array_equal(state_coords, self.env._goal_location)\n",
    "            is_bad = any(np.array_equal(state_coords, bad) for bad in self.env._bad_cells)\n",
    "            \n",
    "            # Draw cell background\n",
    "            rect = pygame.Rect(\n",
    "                pix_sq_size * x,\n",
    "                pix_sq_size * y,\n",
    "                pix_sq_size,\n",
    "                pix_sq_size\n",
    "            )\n",
    "            \n",
    "            if is_goal:\n",
    "                pygame.draw.rect(canvas, (0, 200, 0), rect)  # Green for goal\n",
    "            elif is_bad:\n",
    "                pygame.draw.rect(canvas, (200, 0, 0), rect)  # Red for bad\n",
    "            else:\n",
    "                pygame.draw.rect(canvas, cell_color, rect)\n",
    "            \n",
    "            # Draw grid lines\n",
    "            pygame.draw.rect(canvas, (100, 100, 100), rect, 2)\n",
    "            \n",
    "            # Draw value text\n",
    "            if not (is_goal or is_bad):\n",
    "                value_text = self.tiny_font.render(f\"{V[state_idx]:.1f}\", True, (0, 0, 0))\n",
    "                text_rect = value_text.get_rect(\n",
    "                    center=(x * pix_sq_size + pix_sq_size * 0.5, \n",
    "                           y * pix_sq_size + pix_sq_size * 0.2)\n",
    "                )\n",
    "                canvas.blit(value_text, text_rect)\n",
    "            \n",
    "            # Draw policy arrow (if not terminal)\n",
    "            if not (is_goal or is_bad):\n",
    "                action = policy[state_idx]\n",
    "                self._draw_arrow(canvas, x, y, action, pix_sq_size)\n",
    "        \n",
    "        # Draw goal and bad cell labels\n",
    "        goal_x, goal_y = self.env._goal_location\n",
    "        goal_text = self.small_font.render(\"G\", True, (255, 255, 255))\n",
    "        canvas.blit(goal_text, (goal_x * pix_sq_size + pix_sq_size * 0.4,\n",
    "                                goal_y * pix_sq_size + pix_sq_size * 0.4))\n",
    "        \n",
    "        for bad_cell in self.env._bad_cells:\n",
    "            bad_x, bad_y = bad_cell\n",
    "            bad_text = self.small_font.render(\"X\", True, (255, 255, 255))\n",
    "            canvas.blit(bad_text, (bad_x * pix_sq_size + pix_sq_size * 0.4,\n",
    "                                   bad_y * pix_sq_size + pix_sq_size * 0.4))\n",
    "        \n",
    "        # Draw info panel at bottom\n",
    "        info_y = self.window_size + 10\n",
    "        title_text = self.font.render(f\"{title} - Iteration {iteration}\", True, (0, 0, 0))\n",
    "        canvas.blit(title_text, (10, info_y))\n",
    "        \n",
    "        if delta is not None:\n",
    "            delta_text = self.small_font.render(f\"Delta: {delta:.6f}\", True, (0, 0, 100))\n",
    "            canvas.blit(delta_text, (10, info_y + 50))\n",
    "        \n",
    "        # Update display\n",
    "        self.window.blit(canvas, canvas.get_rect())\n",
    "        pygame.event.pump()\n",
    "        pygame.display.update()\n",
    "        self.clock.tick(2)  # 2 FPS for visualization\n",
    "    \n",
    "    def _draw_arrow(self, canvas, x, y, action, pix_sq_size):\n",
    "        \"\"\"Draw an arrow indicating the action direction.\"\"\"\n",
    "        center_x = x * pix_sq_size + pix_sq_size * 0.5\n",
    "        center_y = y * pix_sq_size + pix_sq_size * 0.6\n",
    "        \n",
    "        arrow_length = pix_sq_size * 0.3\n",
    "        arrow_width = 5\n",
    "        \n",
    "        # Arrow directions\n",
    "        directions = {\n",
    "            0: (arrow_length, 0),      # Right\n",
    "            1: (0, -arrow_length),     # Up\n",
    "            2: (-arrow_length, 0),     # Left\n",
    "            3: (0, arrow_length)       # Down\n",
    "        }\n",
    "        \n",
    "        dx, dy = directions[action]\n",
    "        end_x = center_x + dx\n",
    "        end_y = center_y + dy\n",
    "        \n",
    "        # Draw arrow line\n",
    "        pygame.draw.line(canvas, (0, 0, 0), (center_x, center_y), (end_x, end_y), arrow_width)\n",
    "        \n",
    "        # Draw arrowhead\n",
    "        if action == 0:  # Right\n",
    "            points = [(end_x, end_y), (end_x - 10, end_y - 5), (end_x - 10, end_y + 5)]\n",
    "        elif action == 1:  # Up\n",
    "            points = [(end_x, end_y), (end_x - 5, end_y + 10), (end_x + 5, end_y + 10)]\n",
    "        elif action == 2:  # Left\n",
    "            points = [(end_x, end_y), (end_x + 10, end_y - 5), (end_x + 10, end_y + 5)]\n",
    "        else:  # Down\n",
    "            points = [(end_x, end_y), (end_x - 5, end_y - 10), (end_x + 5, end_y - 10)]\n",
    "        \n",
    "        pygame.draw.polygon(canvas, (0, 0, 0), points)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up pygame resources.\"\"\"\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "print(\"PolicyVisualizer created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28d29a",
   "metadata": {},
   "source": [
    "## 4. Policy Evaluation with Visualization\n",
    "\n",
    "Modified policy evaluation that updates the visualization during convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "de559516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_evaluation_visual function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: POLICY EVALUATION WITH VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def policy_evaluation_visual(env, policy, V, visualizer, iteration, gamma=0.9, theta=10):\n",
    "    \"\"\"\n",
    "    Policy evaluation with real-time visualization of value function updates.\n",
    "    \n",
    "    Shows:\n",
    "    - How values converge iteratively\n",
    "    - Delta (maximum change) decreasing over time\n",
    "    \"\"\"\n",
    "    eval_iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = np.zeros_like(V)\n",
    "        \n",
    "        for state_idx in range(env.size * env.size):\n",
    "            x, y = state_idx % env.size, state_idx // env.size\n",
    "            state_coords = np.array([x, y])\n",
    "            \n",
    "            # Terminal states\n",
    "            is_terminal = False\n",
    "            if np.array_equal(state_coords, env._goal_location):\n",
    "                V_new[state_idx] = 0\n",
    "                is_terminal = True\n",
    "            else:\n",
    "                for bad_cell in env._bad_cells:\n",
    "                    if np.array_equal(state_coords, bad_cell):\n",
    "                        V_new[state_idx] = 0\n",
    "                        is_terminal = True\n",
    "                        break\n",
    "            \n",
    "            if is_terminal:\n",
    "                continue\n",
    "            \n",
    "            # Bellman update\n",
    "            action = policy[state_idx]\n",
    "            v = 0\n",
    "            \n",
    "            movements = {\n",
    "                0: np.array([1, 0]),\n",
    "                1: np.array([0, -1]),\n",
    "                2: np.array([-1, 0]),\n",
    "                3: np.array([0, 1])\n",
    "            }\n",
    "            \n",
    "            outcome_actions = [\n",
    "                \n",
    "                (action - 1) % 4,\n",
    "                action,\n",
    "                (action + 1) % 4\n",
    "            ]\n",
    "            \n",
    "            for i, eff_action in enumerate(outcome_actions):\n",
    "                prob = 0.70 if i == 1 else 0.15\n",
    "                \n",
    "                next_pos = np.clip(\n",
    "                    state_coords + movements[eff_action], \n",
    "                    0, env.size - 1\n",
    "                )\n",
    "                next_state_idx = next_pos[0] + next_pos[1] * env.size\n",
    "                \n",
    "                r = -1.0\n",
    "                if np.array_equal(next_pos, env._goal_location):\n",
    "                    r = 100.0\n",
    "                elif any(np.array_equal(next_pos, bad) for bad in env._bad_cells):\n",
    "                    r = -100.0\n",
    "\n",
    "                v += prob * (r + gamma * V[next_state_idx])\n",
    "            \n",
    "            V_new[state_idx] = v\n",
    "            delta = max(delta, np.abs(V_new[state_idx] - V[state_idx]))\n",
    "        \n",
    "        V = V_new\n",
    "        eval_iteration += 1\n",
    "        \n",
    "        # Visualize every few iterations\n",
    "        if eval_iteration % 3 == 0 or delta < theta:\n",
    "            visualizer.render_policy_and_values(\n",
    "                policy, V, iteration, \n",
    "                title=f\"Policy Evaluation (sweep {eval_iteration})\",\n",
    "                delta=delta\n",
    "            )\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "print(\"policy_evaluation_visual function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac6549",
   "metadata": {},
   "source": [
    "## 5. Policy Improvement with Visualization\n",
    "\n",
    "Modified policy improvement that highlights which states changed their policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "6044fe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_improvement_visual function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: POLICY IMPROVEMENT WITH VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def policy_improvement_visual(env, V, old_policy, visualizer, iteration, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Policy improvement with visualization of changed states.\n",
    "    \n",
    "    Highlights:\n",
    "    - States where the policy action changed\n",
    "    - Before/after comparison\n",
    "    \"\"\"\n",
    "    policy = np.zeros(env.size * env.size, dtype=int)\n",
    "    changed_states = set()\n",
    "    \n",
    "    for state_idx in range(env.size * env.size):\n",
    "        x, y = state_idx % env.size, state_idx // env.size\n",
    "        state_coords = np.array([x, y])\n",
    "        \n",
    "        # Terminal states\n",
    "        if np.array_equal(state_coords, env._goal_location) or \\\n",
    "           any(np.array_equal(state_coords, bad) for bad in env._bad_cells):\n",
    "            policy[state_idx] = 0\n",
    "            continue\n",
    "\n",
    "        # Compute Q-values\n",
    "        q_values = np.zeros(env.action_space.n)\n",
    "        \n",
    "        movements = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, -1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, 1])\n",
    "        }\n",
    "        \n",
    "        for action in range(env.action_space.n):\n",
    "            q = 0\n",
    "            \n",
    "            outcome_actions = [\n",
    "                (action - 1) % 4,\n",
    "                action,\n",
    "                (action + 1) % 4\n",
    "            ]\n",
    "            \n",
    "            for i, eff_action in enumerate(outcome_actions):\n",
    "                prob = 0.70 if i == 1 else 0.15\n",
    "                \n",
    "                next_pos = np.clip(\n",
    "                    state_coords + movements[eff_action], \n",
    "                    0, env.size - 1\n",
    "                )\n",
    "                next_state_idx = next_pos[0] + next_pos[1] * env.size\n",
    "                \n",
    "                r = -1.0\n",
    "                if np.array_equal(next_pos, env._goal_location):\n",
    "                    r = 100.0\n",
    "                elif any(np.array_equal(next_pos, bad) for bad in env._bad_cells):\n",
    "                    r = -100.0\n",
    "\n",
    "                q += prob * (r + gamma * V[next_state_idx])\n",
    "            \n",
    "            q_values[action] = q\n",
    "\n",
    "        # Choose best action\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[state_idx] = best_action\n",
    "        \n",
    "        # Track if policy changed\n",
    "        if best_action != old_policy[state_idx]:\n",
    "            changed_states.add(state_idx)\n",
    "    \n",
    "    # Visualize the improved policy with highlighted changes\n",
    "    visualizer.render_policy_and_values(\n",
    "        policy, V, iteration, changed_states=changed_states,\n",
    "        title=f\"Policy Improvement ({len(changed_states)} states changed)\"\n",
    "    )\n",
    "    time.sleep(1)  # Pause to see the changes\n",
    "    \n",
    "    return policy\n",
    "\n",
    "print(\"policy_improvement_visual function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320abd3",
   "metadata": {},
   "source": [
    "## 6. Policy Iteration with Real-time Visualization\n",
    "\n",
    "Complete Policy Iteration algorithm with step-by-step pygame visualization showing the entire convergence process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "ccb8c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ policy_iteration_with_visualization function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: POLICY ITERATION WITH REAL-TIME VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def policy_iteration_with_visualization(env, gamma=0.95, max_iterations=20):\n",
    "    \"\"\"\n",
    "    Complete Policy Iteration with real-time pygame visualization.\n",
    "    \n",
    "    Shows:\n",
    "    - Initial random policy\n",
    "    - Each evaluation phase (value convergence)\n",
    "    - Each improvement phase (policy changes)\n",
    "    - Final optimal policy\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"POLICY ITERATION WITH REAL-TIME VISUALIZATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nEnvironment Configuration:\")\n",
    "    print(f\"  Goal (Green G): {env._goal_location}\")\n",
    "    print(f\"  Bad Cells (Red X): {env._bad_cells}\")\n",
    "    print(f\"  Discount Factor Î³: {gamma}\")\n",
    "    print(f\"\\nStarting visualization... Watch the pygame window!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize\n",
    "    visualizer = PolicyVisualizer(env)\n",
    "    policy = np.random.choice(env.action_space.n, size=env.size * env.size)\n",
    "    V = np.zeros(env.size * env.size)\n",
    "    \n",
    "    # Show initial random policy\n",
    "    print(\"\\n[Iteration 0] Showing initial RANDOM policy...\")\n",
    "    visualizer.render_policy_and_values(\n",
    "        policy, V, 0, \n",
    "        title=\"Initial Random Policy\"\n",
    "    )\n",
    "    time.sleep(2)\n",
    "    \n",
    "    policy_stable = False\n",
    "    iteration = 0\n",
    "    \n",
    "    # Main Policy Iteration loop\n",
    "    while not policy_stable and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[Iteration {iteration}] POLICY EVALUATION phase...\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # STEP 1: Policy Evaluation (with visualization)\n",
    "        V = policy_evaluation_visual(env, policy, V, visualizer, iteration, gamma)\n",
    "        print(f\"  âœ“ Value function converged!\")\n",
    "        \n",
    "        print(f\"\\n[Iteration {iteration}] POLICY IMPROVEMENT phase...\")\n",
    "        \n",
    "        # STEP 2: Policy Improvement (with visualization)\n",
    "        new_policy = policy_improvement_visual(env, V, policy, visualizer, iteration, gamma)\n",
    "        \n",
    "        # STEP 3: Check convergence\n",
    "        if np.array_equal(new_policy, policy):\n",
    "            policy_stable = True\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"  âœ“âœ“âœ“ CONVERGENCE ACHIEVED! âœ“âœ“âœ“\")\n",
    "            print(f\"  Optimal policy found in {iteration} iterations!\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # Show final optimal policy\n",
    "            visualizer.render_policy_and_values(\n",
    "                new_policy, V, iteration, \n",
    "                title=\"â˜… OPTIMAL POLICY â˜…\"\n",
    "            )\n",
    "        else:\n",
    "            num_changes = np.sum(new_policy != policy)\n",
    "            print(f\"  â†’ {num_changes} states changed, continuing...\")\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    if not policy_stable:\n",
    "        print(f\"\\nâš  Reached maximum iterations ({max_iterations})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Visualization complete! Close the pygame window to continue.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Keep window open until user closes it\n",
    "    running = True\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    visualizer.close()\n",
    "    \n",
    "    return policy, V, iteration\n",
    "\n",
    "print(\"âœ“ policy_iteration_with_visualization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e46e4e",
   "metadata": {},
   "source": [
    "## 7. Interactive Policy Comparison with Custom Rewards\n",
    "\n",
    "Configure custom rewards and see their effect on the entire policy iteration process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "65e6fd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REWARD CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Configure Rewards for Policy Iteration (press Enter for defaults):\n",
      "Current defaults: step=-1, goal=+100, bad=-100, gamma=0.99\n",
      "\n",
      "\n",
      "âœ“ Rewards Configured:\n",
      "  Step Reward:      -1.0\n",
      "  Goal Reward:      +100.0\n",
      "  Bad Cell Penalty: -100.0\n",
      "  Discount Factor:  0.99\n",
      "\n",
      "âœ“ Rewards Configured:\n",
      "  Step Reward:      -1.0\n",
      "  Goal Reward:      +100.0\n",
      "  Bad Cell Penalty: -100.0\n",
      "  Discount Factor:  0.99\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: REWARD CONFIGURATION FOR POLICY ITERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REWARD CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Allow custom reward configuration\n",
    "print(\"\\nðŸ“ Configure Rewards for Policy Iteration (press Enter for defaults):\")\n",
    "print(\"Current defaults: step=-1, goal=+100, bad=-100, gamma=0.99\\n\")\n",
    "\n",
    "try:\n",
    "    step_reward_input = input(\"Step reward (default -1): \").strip()\n",
    "    STEP_REWARD = float(step_reward_input) if step_reward_input else -1.0\n",
    "    \n",
    "    goal_reward_input = input(\"Goal reward (default +100): \").strip()\n",
    "    GOAL_REWARD = float(goal_reward_input) if goal_reward_input else 100.0\n",
    "    \n",
    "    bad_reward_input = input(\"Bad cell penalty (default -100): \").strip()\n",
    "    BAD_REWARD = float(bad_reward_input) if bad_reward_input else -100.0\n",
    "    \n",
    "    gamma_input = input(\"Discount factor gamma (default 0.99): \").strip()\n",
    "    GAMMA = float(gamma_input) if gamma_input else 0.99\n",
    "except:\n",
    "    print(\"Invalid input, using defaults\")\n",
    "    STEP_REWARD = -1.0\n",
    "    GOAL_REWARD = 100.0\n",
    "    BAD_REWARD = -100.0\n",
    "    GAMMA = 0.99\n",
    "\n",
    "print(f\"\\nâœ“ Rewards Configured:\")\n",
    "print(f\"  Step Reward:      {STEP_REWARD:+.1f}\")\n",
    "print(f\"  Goal Reward:      {GOAL_REWARD:+.1f}\")\n",
    "print(f\"  Bad Cell Penalty: {BAD_REWARD:+.1f}\")\n",
    "print(f\"  Discount Factor:  {GAMMA:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "cfe1718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Custom policy evaluation and improvement functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CUSTOM POLICY EVALUATION WITH CONFIGURABLE REWARDS\n",
    "# ============================================================================\n",
    "\n",
    "def policy_evaluation_visual_custom(env, policy, V, visualizer, iteration, gamma=0.99, theta=10):\n",
    "    \"\"\"Policy evaluation with custom rewards from user configuration.\"\"\"\n",
    "    eval_iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = np.zeros_like(V)\n",
    "        \n",
    "        for state_idx in range(env.size * env.size):\n",
    "            x, y = state_idx % env.size, state_idx // env.size\n",
    "            state_coords = np.array([x, y])\n",
    "            \n",
    "            # Terminal states\n",
    "            is_terminal = False\n",
    "            if np.array_equal(state_coords, env._goal_location):\n",
    "                V_new[state_idx] = 0.0\n",
    "                is_terminal = True\n",
    "            else:\n",
    "                for bad_cell in env._bad_cells:\n",
    "                    if np.array_equal(state_coords, bad_cell):\n",
    "                        V_new[state_idx] = 0.0\n",
    "                        is_terminal = True\n",
    "                        break\n",
    "            \n",
    "            if is_terminal:\n",
    "                continue\n",
    "            \n",
    "            # Bellman update with custom rewards\n",
    "            action = policy[state_idx]\n",
    "            v = 0\n",
    "            \n",
    "            movements = {\n",
    "                0: np.array([1, 0]),\n",
    "                1: np.array([0, -1]),\n",
    "                2: np.array([-1, 0]),\n",
    "                3: np.array([0, 1])\n",
    "            }\n",
    "            \n",
    "            outcome_actions = [\n",
    "                (action - 1) % 4,\n",
    "                action,\n",
    "                (action + 1) % 4\n",
    "            ]\n",
    "            \n",
    "            for i, eff_action in enumerate(outcome_actions):\n",
    "                prob = 0.70 if i == 1 else 0.15\n",
    "                \n",
    "                next_pos = np.clip(\n",
    "                    state_coords + movements[eff_action], \n",
    "                    0, env.size - 1\n",
    "                )\n",
    "                next_state_idx = next_pos[0] + next_pos[1] * env.size\n",
    "                \n",
    "                # Use custom rewards\n",
    "                r = STEP_REWARD\n",
    "                if np.array_equal(next_pos, env._goal_location):\n",
    "                    r = GOAL_REWARD\n",
    "                elif any(np.array_equal(next_pos, bad) for bad in env._bad_cells):\n",
    "                    r = BAD_REWARD\n",
    "\n",
    "                v += prob * (r + gamma * V[next_state_idx])\n",
    "            \n",
    "            V_new[state_idx] = v\n",
    "            delta = max(delta, np.abs(V_new[state_idx] - V[state_idx]))\n",
    "        \n",
    "        V = V_new\n",
    "        eval_iteration += 1\n",
    "        \n",
    "        # Visualize every few iterations\n",
    "        if eval_iteration % 3 == 0 or delta < theta:\n",
    "            visualizer.render_policy_and_values(\n",
    "                policy, V, iteration, \n",
    "                title=f\"Policy Evaluation (sweep {eval_iteration})\",\n",
    "                delta=delta\n",
    "            )\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM POLICY IMPROVEMENT WITH CONFIGURABLE REWARDS\n",
    "# ============================================================================\n",
    "\n",
    "def policy_improvement_visual_custom(env, V, old_policy, visualizer, iteration, gamma=0.99):\n",
    "    \"\"\"Policy improvement with custom rewards from user configuration.\"\"\"\n",
    "    policy = np.zeros(env.size * env.size, dtype=int)\n",
    "    changed_states = set()\n",
    "    \n",
    "    for state_idx in range(env.size * env.size):\n",
    "        x, y = state_idx % env.size, state_idx // env.size\n",
    "        state_coords = np.array([x, y])\n",
    "        \n",
    "        # Terminal states\n",
    "        if np.array_equal(state_coords, env._goal_location) or \\\n",
    "           any(np.array_equal(state_coords, bad) for bad in env._bad_cells):\n",
    "            policy[state_idx] = 0\n",
    "            continue\n",
    "\n",
    "        # Compute Q-values with custom rewards\n",
    "        q_values = np.zeros(env.action_space.n)\n",
    "        \n",
    "        movements = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, -1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, 1])\n",
    "        }\n",
    "        \n",
    "        for action in range(env.action_space.n):\n",
    "            q = 0\n",
    "            \n",
    "            outcome_actions = [\n",
    "                (action - 1) % 4,\n",
    "                action,\n",
    "                (action + 1) % 4\n",
    "            ]\n",
    "            \n",
    "            for i, eff_action in enumerate(outcome_actions):\n",
    "                prob = 0.70 if i == 1 else 0.15\n",
    "                \n",
    "                next_pos = np.clip(\n",
    "                    state_coords + movements[eff_action], \n",
    "                    0, env.size - 1\n",
    "                )\n",
    "                next_state_idx = next_pos[0] + next_pos[1] * env.size\n",
    "                \n",
    "                # Use custom rewards\n",
    "                r = STEP_REWARD\n",
    "                if np.array_equal(next_pos, env._goal_location):\n",
    "                    r = GOAL_REWARD\n",
    "                elif any(np.array_equal(next_pos, bad) for bad in env._bad_cells):\n",
    "                    r = BAD_REWARD\n",
    "\n",
    "                q += prob * (r + gamma * V[next_state_idx])\n",
    "            \n",
    "            q_values[action] = q\n",
    "\n",
    "        # Choose best action\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[state_idx] = best_action\n",
    "        \n",
    "        # Track if policy changed\n",
    "        if best_action != old_policy[state_idx]:\n",
    "            changed_states.add(state_idx)\n",
    "    \n",
    "    # Visualize the improved policy with highlighted changes\n",
    "    visualizer.render_policy_and_values(\n",
    "        policy, V, iteration, changed_states=changed_states,\n",
    "        title=f\"Policy Improvement ({len(changed_states)} states changed)\"\n",
    "    )\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "print(\"âœ“ Custom policy evaluation and improvement functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a12a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸŽ® Starting Policy Iteration with Custom Rewards\n",
      "======================================================================\n",
      "\n",
      "Using Rewards:\n",
      "  Step: -1.0, Goal: +100.0, Bad: -100.0, Î³: 0.99\n",
      "\n",
      "Instructions:\n",
      "  - Watch the pygame window to see policy evolution\n",
      "  - Arrows show the action for each state\n",
      "  - Colors show value function (Red=low, Green=high)\n",
      "  - Yellow highlights = states that changed in improvement\n",
      "  - Close the window when done to continue\n",
      "\n",
      "Press Enter to start...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Temp\\ipykernel_13580\\3756624994.py:26: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  self.colormap = cm.get_cmap('RdYlGn')  # Red (low) -> Yellow -> Green (high)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iteration 0] Showing initial RANDOM policy...\n",
      "\n",
      "======================================================================\n",
      "[Iteration 1] POLICY EVALUATION phase...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "[Iteration 1] POLICY EVALUATION phase...\n",
      "======================================================================\n",
      "  âœ“ Value function converged!\n",
      "\n",
      "[Iteration 1] POLICY IMPROVEMENT phase...\n",
      "  âœ“ Value function converged!\n",
      "\n",
      "[Iteration 1] POLICY IMPROVEMENT phase...\n",
      "  â†’ 20 states changed, continuing...\n",
      "\n",
      "======================================================================\n",
      "[Iteration 2] POLICY EVALUATION phase...\n",
      "======================================================================\n",
      "  â†’ 20 states changed, continuing...\n",
      "\n",
      "======================================================================\n",
      "[Iteration 2] POLICY EVALUATION phase...\n",
      "======================================================================\n",
      "  âœ“ Value function converged!\n",
      "\n",
      "[Iteration 2] POLICY IMPROVEMENT phase...\n",
      "  âœ“ Value function converged!\n",
      "\n",
      "[Iteration 2] POLICY IMPROVEMENT phase...\n",
      "  â†’ 9 states changed, continuing...\n",
      "\n",
      "======================================================================\n",
      "[Iteration 3] POLICY EVALUATION phase...\n",
      "======================================================================\n",
      "  â†’ 9 states changed, continuing...\n",
      "\n",
      "======================================================================\n",
      "[Iteration 3] POLICY EVALUATION phase...\n",
      "======================================================================\n",
      "  âœ“ Value function converged!\n",
      "\n",
      "[Iteration 3] POLICY IMPROVEMENT phase...\n",
      "  âœ“ Value function converged!\n",
      "\n",
      "[Iteration 3] POLICY IMPROVEMENT phase...\n",
      "  â†’ 7 states changed, continuing...\n",
      "\n",
      "======================================================================\n",
      "[Iteration 4] POLICY EVALUATION phase...\n",
      "======================================================================\n",
      "  âœ“ Value function converged!\n",
      "\n",
      "[Iteration 4] POLICY IMPROVEMENT phase...\n",
      "  â†’ 7 states changed, continuing...\n",
      "\n",
      "======================================================================\n",
      "[Iteration 4] POLICY EVALUATION phase...\n",
      "======================================================================\n",
      "  âœ“ Value function converged!\n",
      "\n",
      "[Iteration 4] POLICY IMPROVEMENT phase...\n",
      "\n",
      "======================================================================\n",
      "  âœ“âœ“âœ“ CONVERGENCE ACHIEVED! âœ“âœ“âœ“\n",
      "  Optimal policy found in 4 iterations!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Visualization complete! Close the pygame window to continue.\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "  âœ“âœ“âœ“ CONVERGENCE ACHIEVED! âœ“âœ“âœ“\n",
      "  Optimal policy found in 4 iterations!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Visualization complete! Close the pygame window to continue.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RUN INTERACTIVE POLICY ITERATION WITH CUSTOM REWARDS\n",
    "# ============================================================================\n",
    "\n",
    "# Create environment and fix configuration for consistent DP\n",
    "env = GridMazeEnvWithVisualization(render_mode=None, size=5)\n",
    "obs, info = env.reset()  # Fixed seed for reproducibility\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ® Starting Policy Iteration with Custom Rewards\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nUsing Rewards:\")\n",
    "print(f\"  Step: {STEP_REWARD:+.1f}, Goal: {GOAL_REWARD:+.1f}, Bad: {BAD_REWARD:+.1f}, Î³: {GAMMA:.2f}\")\n",
    "print(\"\\nInstructions:\")\n",
    "print(\"  - Watch the pygame window to see policy evolution\")\n",
    "print(\"  - Arrows show the action for each state\")\n",
    "print(\"  - Colors show value function (Red=low, Green=high)\")\n",
    "print(\"  - Yellow highlights = states that changed in improvement\")\n",
    "print(\"  - Close the window when done to continue\")\n",
    "print(\"\\nPress Enter to start...\")\n",
    "input()\n",
    "\n",
    "# Initialize\n",
    "visualizer = PolicyVisualizer(env)\n",
    "policy = np.random.choice(env.action_space.n, size=env.size * env.size)\n",
    "V = np.zeros(env.size * env.size)\n",
    "\n",
    "# Show initial random policy\n",
    "print(\"\\n[Iteration 0] Showing initial RANDOM policy...\")\n",
    "visualizer.render_policy_and_values(\n",
    "    policy, V, 0, \n",
    "    title=\"Initial Random Policy\"\n",
    ")\n",
    "time.sleep(2)\n",
    "\n",
    "policy_stable = False\n",
    "iteration = 0\n",
    "max_iterations = 20\n",
    "\n",
    "# Main Policy Iteration loop\n",
    "while not policy_stable and iteration < max_iterations:\n",
    "    iteration += 1\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[Iteration {iteration}] POLICY EVALUATION phase...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Policy Evaluation with custom rewards\n",
    "    V = policy_evaluation_visual_custom(env, policy, V, visualizer, iteration, GAMMA)\n",
    "    print(f\"  âœ“ Value function converged!\")\n",
    "    \n",
    "    print(f\"\\n[Iteration {iteration}] POLICY IMPROVEMENT phase...\")\n",
    "    \n",
    "    # Policy Improvement with custom rewards\n",
    "    new_policy = policy_improvement_visual_custom(env, V, policy, visualizer, iteration, GAMMA)\n",
    "    \n",
    "    # Check convergence\n",
    "    if np.array_equal(new_policy, policy):\n",
    "        policy_stable = True\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"  âœ“âœ“âœ“ CONVERGENCE ACHIEVED! âœ“âœ“âœ“\")\n",
    "        print(f\"  Optimal policy found in {iteration} iterations!\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Show final optimal policy\n",
    "        visualizer.render_policy_and_values(\n",
    "            new_policy, V, iteration, \n",
    "            title=\"â˜… OPTIMAL POLICY â˜…\"\n",
    "        )\n",
    "    else:\n",
    "        num_changes = np.sum(new_policy != policy)\n",
    "        print(f\"  â†’ {num_changes} states changed, continuing...\")\n",
    "    \n",
    "    policy = new_policy\n",
    "\n",
    "if not policy_stable:\n",
    "    print(f\"\\nâš  Reached maximum iterations ({max_iterations})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Visualization complete! Close the pygame window to continue.\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Keep window open until user closes it\n",
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "    time.sleep(0.1)\n",
    "\n",
    "visualizer.close()\n",
    "\n",
    "# Store results\n",
    "optimal_policy = policy\n",
    "optimal_V = V\n",
    "num_iterations = iteration\n",
    "\n",
    "print(f\"\\nâœ“ Optimal policy computed in {num_iterations} iterations!\")\n",
    "print(f\"\\nFinal Value Function Statistics:\")\n",
    "print(f\"  Maximum Value: {optimal_V.max():.2f}\")\n",
    "print(f\"  Minimum Value: {optimal_V.min():.2f}\")\n",
    "print(f\"  Mean Value: {optimal_V.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612adc9d",
   "metadata": {},
   "source": [
    "## Testing the Optimal Policy with Custom Rewards\n",
    "\n",
    "Now let's test the optimal policy learned with your custom rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING OPTIMAL POLICY WITH CUSTOM REWARDS\n",
      "======================================================================\n",
      "\n",
      "Test Rewards:\n",
      "  Step: -100.0, Goal: +100.0, Bad: -100.0\n",
      "\n",
      "Configuration:\n",
      "  Start: [4 4]\n",
      "  Goal: [2 3]\n",
      "  Bad Cells: [array([0, 0]), array([2, 0])]\n",
      "\n",
      "Running optimal policy...\n",
      "(Note: May fail due to stochastic transitions!)\n",
      "\n",
      "Step  1: Pos=(4,4) â†’ Action=Left â†\n",
      "\n",
      "Configuration:\n",
      "  Start: [4 4]\n",
      "  Goal: [2 3]\n",
      "  Bad Cells: [array([0, 0]), array([2, 0])]\n",
      "\n",
      "Running optimal policy...\n",
      "(Note: May fail due to stochastic transitions!)\n",
      "\n",
      "Step  1: Pos=(4,4) â†’ Action=Left â†\n",
      "Step  2: Pos=(3,4) â†’ Action=Left â†\n",
      "Step  2: Pos=(3,4) â†’ Action=Left â†\n",
      "Step  3: Pos=(3,4) â†’ Action=Left â†\n",
      "Step  3: Pos=(3,4) â†’ Action=Left â†\n",
      "Step  4: Pos=(2,4) â†’ Action=Up â†‘\n",
      "Step  4: Pos=(2,4) â†’ Action=Up â†‘\n",
      "Step  5: Pos=(3,4) â†’ Action=Left â†\n",
      "Step  5: Pos=(3,4) â†’ Action=Left â†\n",
      "Step  6: Pos=(2,4) â†’ Action=Up â†‘\n",
      "Step  6: Pos=(2,4) â†’ Action=Up â†‘\n",
      "\n",
      "==================================================\n",
      "âœ“âœ“âœ“ SUCCESS! Reached the goal! âœ“âœ“âœ“\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "âœ“âœ“âœ“ SUCCESS! Reached the goal! âœ“âœ“âœ“\n",
      "==================================================\n",
      "\n",
      "Results:\n",
      "  Total Reward: -400.0\n",
      "  Steps Taken: 6\n",
      "  Outcome: FAILED\n",
      "\n",
      "Results:\n",
      "  Total Reward: -400.0\n",
      "  Steps Taken: 6\n",
      "  Outcome: FAILED\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TESTING THE OPTIMAL POLICY WITH CUSTOM REWARDS\n",
    "# ============================================================================\n",
    "\n",
    "# Create a custom environment wrapper to use the configured rewards\n",
    "class TestEnvWithCustomRewards(GridMazeEnvWithVisualization):\n",
    "    def step(self, action):\n",
    "        # Call parent step logic with custom rewards\n",
    "        outcome_actions = [\n",
    "            (action - 1) % 4,\n",
    "            action,\n",
    "            (action + 1) % 4\n",
    "        ]\n",
    "        \n",
    "        effective_move_index = self.np_random.choice([0, 1, 2], p=[0.15, 0.70, 0.15])\n",
    "        effective_action = outcome_actions[effective_move_index]\n",
    "\n",
    "        movement = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, -1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, 1])\n",
    "        }[effective_action]\n",
    "        \n",
    "        new_location = np.clip(\n",
    "            self._agent_location + movement, 0, self.size - 1\n",
    "        )\n",
    "        self._agent_location = new_location\n",
    "\n",
    "        terminated = False\n",
    "        reward = STEP_REWARD  # Use custom step reward\n",
    "\n",
    "        if np.array_equal(self._agent_location, self._goal_location):\n",
    "            reward = GOAL_REWARD  # Use custom goal reward\n",
    "            terminated = True\n",
    "        \n",
    "        for bad_cell in self._bad_cells:\n",
    "            if np.array_equal(self._agent_location, bad_cell):\n",
    "                reward = BAD_REWARD  # Use custom bad reward\n",
    "                terminated = True\n",
    "                break\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING OPTIMAL POLICY WITH CUSTOM REWARDS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTest Rewards:\")\n",
    "print(f\"  Step: {STEP_REWARD:+.1f}, Goal: {GOAL_REWARD:+.1f}, Bad: {BAD_REWARD:+.1f}\")\n",
    "\n",
    "# Create test environment with visualization and custom rewards\n",
    "test_env = TestEnvWithCustomRewards(render_mode=\"human\", size=5)\n",
    "obs, info = test_env.reset()\n",
    "\n",
    "# Use same configuration as training\n",
    "test_env._goal_location = env._goal_location\n",
    "test_env._bad_cells = env._bad_cells\n",
    "test_env._agent_location = np.array([4, 4])  # Start from corner\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Start: {test_env._agent_location}\")\n",
    "print(f\"  Goal: {test_env._goal_location}\")\n",
    "print(f\"  Bad Cells: {test_env._bad_cells}\")\n",
    "print(f\"\\nRunning optimal policy...\")\n",
    "print(\"(Note: May fail due to stochastic transitions!)\\n\")\n",
    "\n",
    "action_names = ['Right â†’', 'Up â†‘', 'Left â†', 'Down â†“']\n",
    "total_reward = 0\n",
    "max_steps = 50\n",
    "\n",
    "# Run episode with optimal policy\n",
    "for step in range(max_steps):\n",
    "    x, y = test_env._agent_location\n",
    "    state_idx = x + y * env.size\n",
    "    \n",
    "    action = optimal_policy[state_idx]\n",
    "    print(f\"Step {step+1:2d}: Pos=({x},{y}) â†’ Action={action_names[action]}\")\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    test_env._render_frame()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        if reward == 100.0:\n",
    "            print(\"âœ“âœ“âœ“ SUCCESS! Reached the goal! âœ“âœ“âœ“\")\n",
    "        elif reward == -100.0:\n",
    "            print(\"âœ— Hit a bad cell (due to stochastic transitions)\")\n",
    "        print(f\"{'='*50}\")\n",
    "        time.sleep(2)\n",
    "        break\n",
    "\n",
    "test_env.close()\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total Reward: {total_reward:.1f}\")\n",
    "print(f\"  Steps Taken: {step+1}\")\n",
    "print(f\"  Outcome: {'SUCCESS' if total_reward > 0 else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102b52e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Policy Evaluation Visualization**: Watched values converge iteratively\n",
    "2. **Policy Improvement Visualization**: Saw which states changed their actions\n",
    "3. **Complete Policy Iteration**: Observed convergence to optimal policy over multiple iterations\n",
    "4. **Real-time Rendering**: Used Pygame to show the entire DP process\n",
    "\n",
    "Key observations:\n",
    "- Policy typically converges in 3-5 iterations\n",
    "- Value function converges within each evaluation phase\n",
    "- Changed states (yellow) decrease with each iteration\n",
    "- Final policy is deterministic and optimal for the known MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d6214",
   "metadata": {},
   "source": [
    "## 9. Record Agent Performance and Save Video\n",
    "\n",
    "Now let's record the trained agent navigating the maze and save it as a video file. We'll capture frames and use OpenCV or imageio to create the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d01d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Video recording libraries ready!\n",
      "\n",
      "Preparing to record agent navigation...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 9: VIDEO RECORDING OF AGENT PERFORMANCE\n",
    "# ============================================================================\n",
    "\n",
    "# Install imageio if not already installed\n",
    "try:\n",
    "    import imageio\n",
    "except ImportError:\n",
    "    !pip install imageio imageio-ffmpeg\n",
    "    import imageio\n",
    "\n",
    "print(\"âœ“ Video recording libraries ready!\")\n",
    "print(\"\\nPreparing to record agent navigation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047dabab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording Configuration:\n",
      "  Start Position: [4 4]\n",
      "  Goal Position: [2 3]\n",
      "  Bad Cells: [array([0, 0]), array([2, 0])]\n",
      "  Video Output: maze_agent_recording.mp4\n",
      "\n",
      "==================================================\n",
      "RECORDING AGENT NAVIGATION\n",
      "==================================================\n",
      "Step  1: Pos=(4,4) â†’ Action=LEFT\n",
      "Step  2: Pos=(4,3) â†’ Action=LEFT\n",
      "Step  3: Pos=(3,3) â†’ Action=LEFT\n",
      "Step  4: Pos=(3,4) â†’ Action=LEFT\n",
      "Step  5: Pos=(2,4) â†’ Action=DOWN\n",
      "Step  6: Pos=(1,4) â†’ Action=RIGHT\n",
      "Step  7: Pos=(2,4) â†’ Action=DOWN\n",
      "\n",
      "==================================================\n",
      "âœ“âœ“âœ“ SUCCESS! Reached the goal! âœ“âœ“âœ“\n",
      "==================================================\n",
      "\n",
      "Recording Complete!\n",
      "  Total Reward: 94.0\n",
      "  Steps Taken: 7\n",
      "  Frames Captured: 15\n",
      "  Outcome: SUCCESS\n",
      "Step  6: Pos=(1,4) â†’ Action=RIGHT\n",
      "Step  7: Pos=(2,4) â†’ Action=DOWN\n",
      "\n",
      "==================================================\n",
      "âœ“âœ“âœ“ SUCCESS! Reached the goal! âœ“âœ“âœ“\n",
      "==================================================\n",
      "\n",
      "Recording Complete!\n",
      "  Total Reward: 94.0\n",
      "  Steps Taken: 7\n",
      "  Frames Captured: 15\n",
      "  Outcome: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Create environment in rgb_array mode for video recording\n",
    "video_env = GridMazeEnvWithVisualization(render_mode=\"rgb_array\", size=5)\n",
    "obs, info = video_env.reset()\n",
    "\n",
    "# Use the same maze configuration as before\n",
    "video_env._goal_location = env._goal_location\n",
    "video_env._bad_cells = env._bad_cells\n",
    "video_env._agent_location = np.array([4, 4])  # Start from corner\n",
    "\n",
    "print(\"Recording Configuration:\")\n",
    "print(f\"  Start Position: {video_env._agent_location}\")\n",
    "print(f\"  Goal Position: {video_env._goal_location}\")\n",
    "print(f\"  Bad Cells: {video_env._bad_cells}\")\n",
    "print(f\"  Video Output: maze_agent_recording.mp4\")\n",
    "\n",
    "# Record frames\n",
    "frames = []\n",
    "total_reward = 0\n",
    "max_steps = 50\n",
    "\n",
    "action_names = {0: \"RIGHT\", 1: \"DOWN\", 2: \"LEFT\", 3: \"UP\"}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RECORDING AGENT NAVIGATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Capture initial frame with agent at starting position\n",
    "frame = video_env.render()\n",
    "frames.append(frame)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    # Get current state and action from optimal policy\n",
    "    x, y = video_env._agent_location\n",
    "    state_idx = x + y * video_env.size\n",
    "    action = optimal_policy[state_idx]\n",
    "    \n",
    "    print(f\"Step {step+1:2d}: Pos=({x},{y}) â†’ Action={action_names[action]}\")\n",
    "    \n",
    "    # Take action\n",
    "    obs, reward, terminated, truncated, info = video_env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Capture frame AFTER taking the step\n",
    "    frame = video_env.render()\n",
    "    frames.append(frame)\n",
    "    # Capture frame AFTER taking the step\n",
    "    frame = video_env.render()\n",
    "    frames.append(frame)\n",
    "    \n",
    "    # Check if episode ended\n",
    "    if terminated or truncated:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        if reward == 100.0:\n",
    "            print(\"âœ“âœ“âœ“ SUCCESS! Reached the goal! âœ“âœ“âœ“\")\n",
    "        elif reward == -100.0:\n",
    "            print(\"âœ— Hit a bad cell\")\n",
    "        print(f\"{'='*50}\")\n",
    "        break\n",
    "\n",
    "video_env.close()\n",
    "\n",
    "print(f\"\\nRecording Complete!\")\n",
    "print(f\"  Total Reward: {total_reward:.1f}\")\n",
    "print(f\"  Steps Taken: {step+1}\")\n",
    "print(f\"  Frames Captured: {len(frames)}\")\n",
    "print(f\"  Outcome: {'SUCCESS' if total_reward > 0 else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292e977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving video to: maze_agent_recording.mp4\n",
      "This may take a moment...\n",
      "âœ“ Video saved successfully!\n",
      "âœ“ File: maze_agent_recording.mp4\n",
      "âœ“ Duration: ~7.5 seconds\n",
      "\n",
      "You can now view the video file showing the agent navigating the maze!\n"
     ]
    }
   ],
   "source": [
    "# Save frames as video using imageio\n",
    "output_filename = \"maze_agent_recording.mp4\"\n",
    "\n",
    "print(f\"\\nSaving video to: {output_filename}\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "# Save video at 2 FPS (2 frames per second) for better viewing\n",
    "imageio.mimsave(\n",
    "    output_filename, \n",
    "    frames, \n",
    "    fps=2,  # Slower playback for better visualization\n",
    "    codec='libx264',\n",
    "    quality=8\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Video saved successfully!\")\n",
    "print(f\"âœ“ File: {output_filename}\")\n",
    "print(f\"âœ“ Duration: ~{len(frames)/2:.1f} seconds\")\n",
    "print(f\"\\nYou can now view the video file showing the agent navigating the maze!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1a093",
   "metadata": {},
   "source": [
    "### Optional: Display Video Preview\n",
    "\n",
    "If you want to preview the video in the notebook, you can use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying video: maze_agent_recording.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video controls  width=\"600\" >\n",
       " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the video in the notebook (optional)\n",
    "from IPython.display import Video, display\n",
    "import os\n",
    "\n",
    "if os.path.exists(output_filename):\n",
    "    print(f\"Displaying video: {output_filename}\")\n",
    "    display(Video(output_filename, width=600, embed=True))\n",
    "else:\n",
    "    print(f\"Video file not found: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c468046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REWARD FUNCTION COMPARISON\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š REWARD SCHEME COMPARISON:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Current (Your Setup):\n",
      "  Step Cost:        -1.0\n",
      "  Goal Reward:     100.0\n",
      "  Bad Penalty:    -100.0\n",
      "  Discount (Î³):     0.95\n",
      "  Purpose: High rewards, moderate discount\n",
      "  Effective Planning Horizon: ~20.0 steps\n",
      "  Break-even Steps: ~100.0 (goal worth it if reached within this)\n",
      "\n",
      "Option 1: Balanced (RECOMMENDED):\n",
      "  Step Cost:        -1.0\n",
      "  Goal Reward:      10.0\n",
      "  Bad Penalty:     -10.0\n",
      "  Discount (Î³):     0.99\n",
      "  Purpose: Balanced rewards, high discount - Best for learning\n",
      "  Effective Planning Horizon: ~100.0 steps\n",
      "  Break-even Steps: ~10.0 (goal worth it if reached within this)\n",
      "\n",
      "Option 2: Time-Efficient:\n",
      "  Step Cost:        -0.1\n",
      "  Goal Reward:       1.0\n",
      "  Bad Penalty:      -1.0\n",
      "  Discount (Î³):     0.95\n",
      "  Purpose: Small penalties, encourages exploration\n",
      "  Effective Planning Horizon: ~20.0 steps\n",
      "  Break-even Steps: ~10.0 (goal worth it if reached within this)\n",
      "\n",
      "Option 3: Safety-First:\n",
      "  Step Cost:        -0.5\n",
      "  Goal Reward:      10.0\n",
      "  Bad Penalty:     -15.0\n",
      "  Discount (Î³):     0.99\n",
      "  Purpose: Higher bad cell penalty, promotes caution\n",
      "  Effective Planning Horizon: ~100.0 steps\n",
      "  Break-even Steps: ~20.0 (goal worth it if reached within this)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
