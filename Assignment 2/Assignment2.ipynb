{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14321b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from typing import Tuple, List, Deque\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import wandb\n",
    "\n",
    "# --- 1. Introduction and Setup ---\n",
    "\n",
    "# This notebook implements the DQN and DDQN algorithms based on a more traditional,\n",
    "# function-oriented style inspired by PyTorch tutorials. It is adapted to fulfill\n",
    "# the assignment requirements, including training on multiple environments and\n",
    "# running comprehensive evaluations.\n",
    "\n",
    "# --- Device Detection ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# --- WandB Setup ---\n",
    "USE_WANDB = True\n",
    "PROJECT_NAME = \"RL-Assignment2-EVALS1\"\n",
    "\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        # Fix for Windows environments to prevent symlink errors\n",
    "        os.environ['WANDB_CONSOLE'] = 'off'\n",
    "        \n",
    "        # Read API key from a local file\n",
    "        with open(\"key.txt\", \"r\") as f:\n",
    "            api_key = f.read().strip()\n",
    "        wandb.login(key=api_key)\n",
    "        print(\"WandB login successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not log in to WandB: {e}\")\n",
    "        USE_WANDB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9a60b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Hyperparameters and Configuration\n",
    "\n",
    "This cell centralizes all tunable parameters for the experiments. The assignment requires testing different values for these to find the best setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4529619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the AdamW optimizer\n",
    "# REPLAY_MEMORY_SIZE is the maximum size of the replay buffer\n",
    "\n",
    "# --- Default Configuration ---\n",
    "# These are baseline values. Environment-specific configs below will override them.\n",
    "default_config = {\n",
    "    \"USE_DDQN\": True,\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"EPS_START\": 0.9,\n",
    "    \"EPS_END\": 0.05,\n",
    "    \"EPS_DECAY\": 1000,\n",
    "    \"TAU\": 0.005,\n",
    "    \"LR\": 5e-4,\n",
    "    \"REPLAY_MEMORY_SIZE\": 10000,\n",
    "    \"NUM_EPISODES\": 600,\n",
    "    \"TARGET_UPDATE\": 10,\n",
    "    \"N_BINS\": 16\n",
    "}\n",
    "\n",
    "# --- Environment-Specific Configurations ---\n",
    "# Here, we override the default config for each environment to optimize performance.\n",
    "env_configs = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"BATCH_SIZE\": 64,\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"EPS_START\": 0.9,\n",
    "        \"EPS_END\": 0.05,\n",
    "        \"EPS_DECAY\": 200,\n",
    "        \"TAU\": 0.005,\n",
    "        \"LR\": 1e-3,\n",
    "        \"REPLAY_MEMORY_SIZE\": 10000,\n",
    "        \"NUM_EPISODES\": 600,\n",
    "        \"TARGET_UPDATE\": 4\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"BATCH_SIZE\": 128,\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"EPS_START\": 1.0,\n",
    "        \"EPS_END\": 0.01,\n",
    "        \"EPS_DECAY\": 1000,\n",
    "        \"TAU\": 0.005,\n",
    "        \"LR\": 5e-4,\n",
    "        \"REPLAY_MEMORY_SIZE\": 50000,\n",
    "        \"NUM_EPISODES\": 1000,\n",
    "        \"TARGET_UPDATE\": 10\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"BATCH_SIZE\": 256,\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"EPS_START\": 0.9,\n",
    "        \"EPS_END\": 0.05,\n",
    "        \"EPS_DECAY\": 10000,\n",
    "        \"TAU\": 0.005,\n",
    "        \"LR\": 5e-4,\n",
    "        \"REPLAY_MEMORY_SIZE\": 1000,\n",
    "        \"NUM_EPISODES\": 1500,\n",
    "        \"TARGET_UPDATE\": 10\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"BATCH_SIZE\": 256,\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"EPS_START\": 1.0,\n",
    "        \"EPS_END\": 0.05,\n",
    "        \"EPS_DECAY\": 500,\n",
    "        \"TAU\": 0.005,\n",
    "        \"LR\": 5e-4,\n",
    "        \"REPLAY_MEMORY_SIZE\": 100000,\n",
    "        \"NUM_EPISODES\": 500,\n",
    "        \"TARGET_UPDATE\": 10,\n",
    "        \"N_BINS\": 5\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965beae",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Core DQN Components\n",
    "\n",
    "This section defines the three fundamental building blocks of the agent:\n",
    "1.  **DQN Model**: A simple feed-forward neural network that estimates Q-values.\n",
    "2.  **Replay Memory**: A buffer that stores past experiences (`state`, `action`, `reward`, `next_state`) so the agent can learn from them in batches. This decorrelates experiences and stabilizes training.\n",
    "3.  **Transition**: A `namedtuple` for conveniently storing a single experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1694069",
   "metadata": {},
   "source": [
    "---\n",
    "### Discretization Wrapper for Continuous Environments\n",
    "\n",
    "Since our DQN agent can only output discrete actions (e.g., action 0, 1, 2), it cannot directly handle environments with continuous action spaces like `Pendulum-v1`.\n",
    "\n",
    "To solve this, we create a **wrapper** class. This class sits \"on top\" of the original environment and modifies its behavior. The `DiscretizeActionWrapper` does the following:\n",
    "1.  It takes the continuous action space (e.g., a range from -2.0 to 2.0 for Pendulum) and converts it into a fixed number of discrete actions (e.g., 5 bins).\n",
    "2.  It tells our agent that there are now 5 possible actions.\n",
    "3.  When our agent picks a discrete action (e.g., action `2`), the wrapper translates it back into the corresponding continuous value (e.g., `0.0`) before passing it to the actual environment.\n",
    "\n",
    "This allows us to use our DQN agent on `Pendulum-v1` without changing the agent's core logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizeActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper to discretize a continuous action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_bins):\n",
    "        super().__init__(env)\n",
    "        self.n_bins = n_bins\n",
    "        self.action_space = gym.spaces.Discrete(n_bins)\n",
    "        self.continuous_action_space = env.action_space\n",
    "        \n",
    "        # Create a mapping from discrete actions to continuous values\n",
    "        self.action_map = np.linspace(\n",
    "            self.continuous_action_space.low[0],\n",
    "            self.continuous_action_space.high[0],\n",
    "            n_bins\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the discrete action to a continuous action\n",
    "        continuous_action = np.array([self.action_map[action]], dtype=np.float32)\n",
    "        return self.env.step(continuous_action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921d9a3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training and Optimization Functions\n",
    "\n",
    "This section contains the core logic for the agent's learning process.\n",
    "\n",
    "- **`select_action`**: Implements an epsilon-greedy policy. With probability `epsilon`, it takes a random action (exploration). Otherwise, it takes the action with the highest predicted Q-value (exploitation).\n",
    "- **`optimize_model`**: This is the heart of the learning algorithm. It samples a batch of experiences from the replay memory and computes the loss. It supports both standard DQN and Double DQN (DDQN) based on the `USE_DDQN` flag in the config.\n",
    "- **`get_env_details`**: A helper to get the action and observation space sizes from an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, env, policy_net, n_actions, config, steps_done):\n",
    "    \"\"\"\n",
    "    Select action using epsilon-greedy policy.\n",
    "    Returns: action tensor, updated steps_done\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    eps_threshold = config[\"EPS_END\"] + (config[\"EPS_START\"] - config[\"EPS_END\"]) * \\\n",
    "        math.exp(-1. * steps_done / config[\"EPS_DECAY\"])\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1), steps_done\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long), steps_done\n",
    "\n",
    "def optimize_model(memory, policy_net, target_net, optimizer, config):\n",
    "    if len(memory) < config[\"BATCH_SIZE\"]:\n",
    "        return None\n",
    "    transitions = memory.sample(config[\"BATCH_SIZE\"])\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(config[\"BATCH_SIZE\"], device=device)\n",
    "    with torch.no_grad():\n",
    "        if config[\"USE_DDQN\"]:\n",
    "            # DDQN: Use policy_net to select actions, and target_net to evaluate them\n",
    "            best_actions = policy_net(non_final_next_states).argmax(1).unsqueeze(-1)\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, best_actions).squeeze()\n",
    "        else:\n",
    "            # Standard DQN: Use target_net for both selection and evaluation\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "            \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * config[\"GAMMA\"]) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be86d49",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Main Training and Evaluation Loop\n",
    "\n",
    "This is the final, runnable part of the notebook. It automates the entire process as required by the assignment:\n",
    "1.  It defines the list of environments to be trained on.\n",
    "2.  It loops through both **DQN and DDQN** algorithms.\n",
    "3.  For each algorithm and each environment, it:\n",
    "    *   Initializes the policy and target networks, optimizer, and replay memory.\n",
    "    *   Runs the main training loop for a set number of episodes.\n",
    "    *   Logs metrics to WandB (if enabled).\n",
    "    *   Updates the target network periodically.\n",
    "    *   Saves the final trained model.\n",
    "4.  After training, it runs a **100-episode evaluation** to test the agent's performance and prints the average reward.\n",
    "5.  Finally, it prints a summary of all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environments to run ---\n",
    "environments = [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\", \"Pendulum-v1\"]\n",
    "evaluation_results = {}\n",
    "\n",
    "# --- Main Experiment Loop ---\n",
    "for use_ddqn_flag in [False, True]:\n",
    "    algo_name = \"DDQN\" if use_ddqn_flag else \"DQN\"\n",
    "    print(f\"\\n{'='*40}\\nRunning Experiment with: {algo_name}\\n{'='*40}\\n\")\n",
    "    \n",
    "    if algo_name not in evaluation_results:\n",
    "        evaluation_results[algo_name] = {}\n",
    "\n",
    "    for env_name in environments:\n",
    "        # --- Create a combined configuration for the current environment ---\n",
    "        current_config = default_config.copy()\n",
    "        if env_name in env_configs:\n",
    "            current_config.update(env_configs[env_name])\n",
    "        current_config[\"USE_DDQN\"] = use_ddqn_flag\n",
    "\n",
    "        model_path = f\"./models/{algo_name}_{env_name}_policy.pth\"\n",
    "        run_name = f\"{algo_name}_{env_name}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "        # --- Environment Setup ---\n",
    "        is_continuous = False\n",
    "        temp_env = gym.make(env_name)\n",
    "        if isinstance(temp_env.action_space, gym.spaces.Box):\n",
    "            is_continuous = True\n",
    "        temp_env.close()\n",
    "\n",
    "        if is_continuous:\n",
    "            print(f\"--- Detected continuous action space for {env_name}. Applying discretization wrapper. ---\")\n",
    "            env = DiscretizeActionWrapper(gym.make(env_name), n_bins=current_config[\"N_BINS\"])\n",
    "        else:\n",
    "            env = gym.make(env_name)\n",
    "            \n",
    "        n_actions = env.action_space.n\n",
    "        state, _ = env.reset()\n",
    "        n_observations = len(state)\n",
    "        env.reset()\n",
    "\n",
    "        policy_net = DQN(n_observations, n_actions).to(device)\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"--- Found pre-trained model for {algo_name} on {env_name} ---\")\n",
    "            print(f\"Loading model from {model_path} and skipping training.\\n\")\n",
    "            policy_net.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            print(f\"--- Training {algo_name} on {env_name} ---\")\n",
    "            target_net = DQN(n_observations, n_actions).to(device)\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            optimizer = optim.AdamW(policy_net.parameters(), lr=current_config[\"LR\"], amsgrad=True)\n",
    "            memory = ReplayMemory(current_config[\"REPLAY_MEMORY_SIZE\"])\n",
    "            \n",
    "            steps_done = 0  # Reset for each training run\n",
    "            \n",
    "            if USE_WANDB:\n",
    "                if wandb.run is not None: wandb.finish()\n",
    "                wandb.init(project=PROJECT_NAME, name=run_name, config=current_config, reinit=True)\n",
    "\n",
    "            num_episodes = current_config[\"NUM_EPISODES\"]\n",
    "            episode_rewards_train = []\n",
    "            episode_losses = []\n",
    "            \n",
    "            for i_episode in range(num_episodes):\n",
    "                state, info = env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                ep_reward = 0\n",
    "                ep_losses = []\n",
    "                \n",
    "                for t in range(1500):\n",
    "                    action, steps_done = select_action(state, env, policy_net, n_actions, current_config, steps_done)\n",
    "                    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                    ep_reward += reward\n",
    "                    reward = torch.tensor([reward], device=device)\n",
    "                    done = terminated or truncated\n",
    "\n",
    "                    next_state = None if terminated else torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                    memory.push(state, action, next_state, reward)\n",
    "                    state = next_state\n",
    "\n",
    "                    loss = optimize_model(memory, policy_net, target_net, optimizer, current_config)\n",
    "                    if loss is not None:\n",
    "                        ep_losses.append(loss)\n",
    "                    \n",
    "                    # Soft update of target network every step (as per your original design)\n",
    "                    # Alternative: Hard update every N episodes (uncomment below)\n",
    "                    if t % current_config[\"TARGET_UPDATE\"] == 0:\n",
    "                        target_net_state_dict = target_net.state_dict()\n",
    "                        policy_net_state_dict = policy_net.state_dict()\n",
    "                        for key in policy_net_state_dict:\n",
    "                            target_net_state_dict[key] = policy_net_state_dict[key]*current_config[\"TAU\"] + target_net_state_dict[key]*(1-current_config[\"TAU\"])\n",
    "                        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                episode_rewards_train.append(ep_reward)\n",
    "                avg_loss = sum(ep_losses) / len(ep_losses) if ep_losses else 0\n",
    "                episode_losses.append(avg_loss)\n",
    "                \n",
    "                if USE_WANDB and avg_loss > 0:\n",
    "                    wandb.log({\n",
    "                        \"train/reward\": ep_reward, \n",
    "                        \"train/loss\": avg_loss,\n",
    "                        \"train/epsilon\": current_config[\"EPS_END\"] + (current_config[\"EPS_START\"] - current_config[\"EPS_END\"]) * math.exp(-1. * steps_done / current_config[\"EPS_DECAY\"])\n",
    "                    }, step=i_episode)\n",
    "                \n",
    "                if i_episode % 50 == 0:\n",
    "                    print(f\"  Episode {i_episode}/{num_episodes} | Reward: {ep_reward}\")\n",
    "\n",
    "            print(\"--- Training Complete ---\")\n",
    "            torch.save(policy_net.state_dict(), model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "            if USE_WANDB and wandb.run is not None:\n",
    "                wandb.finish()\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        print(f\"--- Evaluating {algo_name} on {env_name} for 100 episodes (with video recording) ---\")\n",
    "        \n",
    "        video_folder = f\"./videos/{algo_name}_{env_name}/\"\n",
    "        os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "        if is_continuous:\n",
    "            eval_env = DiscretizeActionWrapper(gym.make(env_name, render_mode=\"rgb_array\"), n_bins=current_config[\"N_BINS\"])\n",
    "        else:\n",
    "            eval_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        \n",
    "        eval_env = gym.wrappers.RecordVideo(\n",
    "            eval_env, \n",
    "            video_folder,\n",
    "            episode_trigger=lambda x: x % 25 == 0,\n",
    "            name_prefix=f\"{algo_name}-{env_name}\"\n",
    "        )\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_durations = []\n",
    "        for i in range(100):\n",
    "            state, _ = eval_env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            ep_reward = 0\n",
    "            ep_duration = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(state).max(1)[1].view(1, 1)\n",
    "                obs, reward, terminated, truncated, _ = eval_env.step(action.item())\n",
    "                ep_reward += reward\n",
    "                ep_duration += 1\n",
    "                done = terminated or truncated\n",
    "                if not done:\n",
    "                    state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            episode_rewards.append(ep_reward)\n",
    "            episode_durations.append(ep_duration)\n",
    "        \n",
    "        avg_reward = sum(episode_rewards) / 100\n",
    "        evaluation_results[algo_name][env_name] = avg_reward\n",
    "        print(f\"Average Reward over 100 episodes: {avg_reward:.2f}\")\n",
    "        print(f\"Videos saved in: {video_folder}\\n\")\n",
    "        \n",
    "        # Log evaluation data to WandB\n",
    "        if USE_WANDB:\n",
    "            # Initialize wandb for evaluation logging (in case training was skipped)\n",
    "            if wandb.run is None:\n",
    "                wandb.init(project=PROJECT_NAME, name=f\"{run_name}_eval\", config=current_config, reinit=True)\n",
    "            \n",
    "            # Create a wandb.Table\n",
    "            eval_table = wandb.Table(columns=[\"Episode\", \"Reward\", \"Duration\"])\n",
    "            for i in range(100):\n",
    "                eval_table.add_data(i + 1, episode_rewards[i], episode_durations[i])\n",
    "            \n",
    "            # Log the table and summary metrics\n",
    "            wandb.log({\n",
    "                \"evaluation/avg_reward\": avg_reward,\n",
    "                \"evaluation/std_reward\": np.std(episode_rewards),\n",
    "                \"evaluation/max_reward\": max(episode_rewards),\n",
    "                \"evaluation/min_reward\": min(episode_rewards),\n",
    "                \"evaluation/avg_duration\": sum(episode_durations) / 100,\n",
    "                \"evaluation/evaluation_table\": eval_table\n",
    "            })\n",
    "            \n",
    "            if wandb.run is not None:\n",
    "                wandb.finish()\n",
    "\n",
    "        eval_env.close()\n",
    "        env.close()\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\\n--- Overall Evaluation Summary ---\")\n",
    "for algo_name, results in evaluation_results.items():\n",
    "    print(f\"\\n--- {algo_name} Results ---\")\n",
    "    for env_name, avg_reward in results.items():\n",
    "        print(f\"  Environment: {env_name} | Average Reward (100 eps): {avg_reward:.2f}\")\n",
    "print(\"------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
