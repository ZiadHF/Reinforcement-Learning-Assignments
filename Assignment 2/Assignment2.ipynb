{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ca0969",
   "metadata": {},
   "source": [
    "# Assignment 2 â€” DQN (in-place implementation)\n",
    "\n",
    "This notebook implements a complete, well-documented DQN agent for OpenAI/Gymnasium environments (default: CartPole-v1).\n",
    "It includes: environment setup, a replay buffer, Q-network, target network updates, training loop, evaluation, and Weights & Biases (WandB) logging.\n",
    "\n",
    "Do not run any cells here unless you want to (you said you'll run tests). The cells are arranged so you can run them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1545ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment to run)\n",
    "# !pip install gymnasium torch wandb numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4801f",
   "metadata": {},
   "source": [
    "## Imports and configuration\n",
    "Small, self-contained utilities and device detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc794cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from typing import Tuple, List, Deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import wandb\n",
    "\n",
    "# Device detection\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2813d9",
   "metadata": {},
   "source": [
    "## WandB login and experiment configuration\n",
    "This cell reads an API key from `key.txt` (present in the folder) and initializes a WandB run.\n",
    "If you prefer not to use WandB, set `USE_WANDB = False` below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2adaa9",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Configuration\n",
    "\n",
    "This cell contains all the hyperparameters for the DQN agent.\n",
    "You can tweak the values here to see how they affect training performance for each environment. The assignment suggests experimenting with:\n",
    "- `gamma` (Discount Factor)\n",
    "- `epsilon_decay` (Epsilon Decay Rate)\n",
    "- `lr` (NN Learning Rate)\n",
    "- `buffer_size` (Replay Memory Size)\n",
    "- `batch_size` (Learning Batch Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098b7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Base Hyperparameter Configuration ---\n",
    "# You can copy and modify this dictionary for each environment.\n",
    "config = {\n",
    "    # --- Agent ---\n",
    "    \"use_ddqn\": True,             # Set to True to use Double DQN, False for standard DQN\n",
    "\n",
    "    # --- Environment ---\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # --- Training ---\n",
    "    \"max_episodes\": 500,          # Number of episodes to train for\n",
    "    \"max_steps\": 1000,            # Max steps per episode\n",
    "    \"start_training_after\": 1000, # Steps to collect before training starts\n",
    "\n",
    "    # --- DQN Agent ---\n",
    "    \"gamma\": 0.99,                # Discount factor\n",
    "    \"lr\": 1e-4,                   # Learning rate for the Adam optimizer\n",
    "    \"buffer_size\": 10000,         # Max size of the replay buffer\n",
    "    \"batch_size\": 64,             # Number of samples to train on from the buffer\n",
    "    \"target_update\": 250,         # Steps between updating the target network\n",
    "\n",
    "    # --- Epsilon-Greedy Exploration ---\n",
    "    \"epsilon_start\": 1.0,         # Starting value of epsilon\n",
    "    \"epsilon_final\": 0.05,        # Final value of epsilon\n",
    "    \"epsilon_decay\": 10000,       # Epsilon decay rate\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf46f69",
   "metadata": {},
   "source": [
    "## Utilities: seeding and helpers\n",
    "Deterministic seeds (best-effort) and a small helper for epsilon schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa63dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int, env=None):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    if env is not None:\n",
    "        try:\n",
    "            env.reset(seed=seed)\n",
    "            env.action_space.seed(seed)\n",
    "            env.observation_space.seed(seed)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def linear_epsilon(start, final, decay, step):\n",
    "    return final + (start - final) * np.exp(-1. * step / decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87415a99",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "A simple deque-based experience replay with sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998d1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer: Deque[Transition] = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states = torch.tensor([b.state for b in batch], dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor([b.action for b in batch], dtype=torch.int64, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor([b.reward for b in batch], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.tensor([b.next_state for b in batch], dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor([b.done for b in batch], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe21f21",
   "metadata": {},
   "source": [
    "## Q-network\n",
    "A small MLP suitable for low-dimensional observations (e.g., CartPole)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d415d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_size: int, n_actions: int, hidden_sizes=(128, 128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_size = obs_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_size, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_size = h\n",
    "        layers.append(nn.Linear(in_size, n_actions))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c119e8",
   "metadata": {},
   "source": [
    "## DQN Agent\n",
    "Includes epsilon-greedy policy, optimization step, and target network updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaffd45",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluation Loop\n",
    "\n",
    "This section contains the core `train` and `evaluate` functions.\n",
    "\n",
    "- **`train()`**: This function takes an agent and an environment, runs the main training loop, and logs the results. It now saves the trained model to a specific path for each environment.\n",
    "- **`evaluate()`**: This function takes a trained agent and an environment and runs a set number of episodes with a greedy policy (no exploration) to measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27460e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, cfg, run_name):\n",
    "    \"\"\"\n",
    "    Train a DQN agent on a given environment.\n",
    "\n",
    "    Args:\n",
    "        agent (DQNAgent): The agent to train.\n",
    "        env (gym.Env): The environment to train on.\n",
    "        cfg (dict): The hyperparameter configuration.\n",
    "        run_name (str): The name for the WandB run.\n",
    "    \"\"\"\n",
    "    if USE_WANDB:\n",
    "        # Ensure any previous run is finished\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish()\n",
    "        # Start a new run\n",
    "        wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            name=run_name,\n",
    "            config=cfg,\n",
    "            reinit=True\n",
    "        )\n",
    "\n",
    "    total_steps = 0\n",
    "    print(f\"--- Starting Training for {cfg['env_name']} ---\")\n",
    "\n",
    "    for episode in range(1, cfg['max_episodes'] + 1):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0.0\n",
    "        \n",
    "        for t in range(cfg['max_steps']):\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, float(done))\n",
    "            loss = None\n",
    "            if agent.steps_done > cfg['start_training_after']:\n",
    "                loss = agent.optimize()\n",
    "            \n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            total_steps += 1\n",
    "\n",
    "            if total_steps % cfg['target_update'] == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Logging\n",
    "        log_stats = {'episode': episode, 'reward': ep_reward, 'total_steps': total_steps}\n",
    "        if loss is not None:\n",
    "            log_stats['loss'] = loss\n",
    "        \n",
    "        if USE_WANDB:\n",
    "            wandb.log(log_stats)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"  Episode {episode}/{cfg['max_episodes']} | Reward: {ep_reward:.2f}\")\n",
    "\n",
    "    print(f\"--- Training Finished for {cfg['env_name']} ---\")\n",
    "    \n",
    "    # Save the trained policy\n",
    "    model_path = f\"dqn_{cfg['env_name']}.pth\"\n",
    "    torch.save(agent.policy_net.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\\n\")\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.finish()\n",
    "\n",
    "def evaluate(agent, env, n_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a trained agent on an environment.\n",
    "    \"\"\"\n",
    "    print(f\"--- Evaluating for {env.spec.id} over {n_episodes} episodes ---\")\n",
    "    agent.policy_net.eval()  # Set network to evaluation mode\n",
    "    \n",
    "    total_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            action = agent.select_action(state, training=False)  # Greedy action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "        total_rewards.append(ep_reward)\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"  Average Reward: {avg_reward:.2f}\\n\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70b093",
   "metadata": {},
   "source": [
    "## 8. Main Execution Block\n",
    "\n",
    "This is the final, runnable part of the notebook. It will:\n",
    "1.  Define the list of environments to be trained and tested on.\n",
    "2.  Loop through each environment.\n",
    "3.  For each one, it creates the environment and a new DQN agent.\n",
    "4.  Calls `train()` to train the agent on that specific environment.\n",
    "5.  Calls `evaluate()` to test the newly trained agent for 100 episodes.\n",
    "6.  Stores and prints the final evaluation results.\n",
    "\n",
    "**This is the only cell you need to run to perform all training and testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e1ea525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Running Experiment with: DQN\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gymnasium' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning Experiment with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m env_name \u001b[38;5;129;01min\u001b[39;00m environments:\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# 1. Create the environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     env = \u001b[43mgymnasium\u001b[49m.make(env_name)\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# 2. Update the config for the current environment and algorithm\u001b[39;00m\n\u001b[32m     21\u001b[39m     current_config = config.copy()\n",
      "\u001b[31mNameError\u001b[39m: name 'gymnasium' is not defined"
     ]
    }
   ],
   "source": [
    "# --- List of Environments to Run ---\n",
    "environments = [\n",
    "    \"CartPole-v1\",\n",
    "    \"Acrobot-v1\",\n",
    "    \"MountainCar-v0\",\n",
    "]\n",
    "\n",
    "# --- Store final evaluation results ---\n",
    "evaluation_results = {}\n",
    "\n",
    "# --- Main Loop ---\n",
    "for use_ddqn_flag in [False, True]: # Run for both DQN and DDQN\n",
    "    algo_name = \"DDQN\" if use_ddqn_flag else \"DQN\"\n",
    "    print(f\"\\n{'='*40}\\nRunning Experiment with: {algo_name}\\n{'='*40}\\n\")\n",
    "\n",
    "    for env_name in environments:\n",
    "        # 1. Create the environment\n",
    "        env = gymnasium.make(env_name)\n",
    "        \n",
    "        # 2. Update the config for the current environment and algorithm\n",
    "        current_config = config.copy()\n",
    "        current_config[\"env_name\"] = env_name\n",
    "        current_config[\"use_ddqn\"] = use_ddqn_flag\n",
    "        \n",
    "        # Adjust hyperparameters for specific environments if needed\n",
    "        if env_name == \"Acrobot-v1\":\n",
    "            current_config[\"max_episodes\"] = 1000\n",
    "            current_config[\"lr\"] = 5e-4\n",
    "            current_config[\"target_update\"] = 500\n",
    "        elif env_name == \"MountainCar-v0\":\n",
    "            current_config[\"max_episodes\"] = 2000\n",
    "            current_config[\"epsilon_decay\"] = 30000\n",
    "            current_config[\"start_training_after\"] = 2000\n",
    "            current_config[\"lr\"] = 1e-3\n",
    "\n",
    "        # 3. Create the agent\n",
    "        obs_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = DQNAgent(obs_size, action_size, current_config)\n",
    "\n",
    "        # 4. Set a unique run name for WandB\n",
    "        run_name = f\"{algo_name}_{env_name}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "        # 5. Train the agent\n",
    "        set_seed(current_config['seed'], env)\n",
    "        train(agent, env, current_config, run_name)\n",
    "\n",
    "        # 6. Evaluate the trained agent\n",
    "        avg_reward = evaluate(agent, env, n_episodes=100)\n",
    "        \n",
    "        # Store results in a structured way\n",
    "        if algo_name not in evaluation_results:\n",
    "            evaluation_results[algo_name] = {}\n",
    "        evaluation_results[algo_name][env_name] = avg_reward\n",
    "        \n",
    "        # 7. Close the environment\n",
    "        env.close()\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\\n--- Overall Evaluation Summary ---\")\n",
    "for algo_name, results in evaluation_results.items():\n",
    "    print(f\"\\n--- {algo_name} Results ---\")\n",
    "    for env_name, avg_reward in results.items():\n",
    "        print(f\"  Environment: {env_name} | Average Reward (100 eps): {avg_reward:.2f}\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, obs_size, n_actions, cfg):\n",
    "        self.obs_size = obs_size\n",
    "        self.n_actions = n_actions\n",
    "        self.cfg = cfg\n",
    "        self.use_ddqn = cfg.get(\"use_ddqn\", False) # Default to standard DQN\n",
    "\n",
    "        self.policy_net = QNetwork(obs_size, n_actions).to(device)\n",
    "        self.target_net = QNetwork(obs_size, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg['lr'])\n",
    "        self.replay = ReplayBuffer(cfg['buffer_size'])\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, training=True):\n",
    "        # state: np.array or torch tensor (1d)\n",
    "        epsilon = linear_epsilon(self.cfg['epsilon_start'], self.cfg['epsilon_final'], self.cfg['epsilon_decay'], self.steps_done)\n",
    "        \n",
    "        # Only increment steps during training\n",
    "        if training:\n",
    "            self.steps_done += 1\n",
    "\n",
    "        if training and random.random() < epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                qvals = self.policy_net(s)\n",
    "                return int(qvals.argmax(dim=1).item())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.replay) < self.cfg['batch_size']:\n",
    "            return None\n",
    "        states, actions, rewards, next_states, dones = self.replay.sample(self.cfg['batch_size'])\n",
    "\n",
    "        # Q(s,a) - The Q-value for the action that was actually taken\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.use_ddqn:\n",
    "                # --- Double DQN ---\n",
    "                # 1. Select the best action a' from the *policy* network for the next state s'\n",
    "                next_actions = self.policy_net(next_states).argmax(dim=1).unsqueeze(1)\n",
    "                # 2. Evaluate that action a' using the *target* network to get Q(s', a')\n",
    "                next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            else:\n",
    "                # --- Standard DQN ---\n",
    "                # Select the max Q-value from the target network for the next state\n",
    "                next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            \n",
    "            # Calculate the target Q-value: r + gamma * Q_target(s', a') * (1 - done)\n",
    "            target = rewards + (1 - dones) * (self.cfg['gamma'] * next_q_values)\n",
    "\n",
    "        loss = nn.functional.mse_loss(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0) # Clip gradients\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c47ddd",
   "metadata": {},
   "source": [
    "## Training loop (no execution)\n",
    "This cell contains the train() function which runs episodes, logs metrics, and updates networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4e6c0",
   "metadata": {},
   "source": [
    "## Evaluation helper\n",
    "Run deterministic episodes using the policy network (no exploration)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f456745",
   "metadata": {},
   "source": [
    "## How to run (suggested)\n",
    "Run the following steps in order in your environment (PowerShell on Windows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c5c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ready. Run the cells step-by-step in your environment.\n"
     ]
    }
   ],
   "source": [
    "# Example (do not run automatically in this notebook):\n",
    "# 1) Create and activate a virtualenv (recommended)\n",
    "# powershell: python -m venv .venv; .\\.venv\\Scripts\\Activate.ps1\n",
    "# 2) Install dependencies\n",
    "# pip install gymnasium torch wandb numpy\n",
    "# 3) (Optional) edit `config` dictionary above for hyperparameters\n",
    "# 4) Run training:\n",
    "# result = train()\n",
    "# 5) Evaluate:\n",
    "# rewards = evaluate(torch.load('dqn_policy.pth'))\n",
    "# print(rewards)\n",
    "\n",
    "print('Notebook ready. Run the cells step-by-step in your environment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65883701",
   "metadata": {},
   "source": [
    "## Notes and tips\n",
    "- The default configuration uses CartPole-v1; change `config['env_name']` to try other Gymnasium envs.\n",
    "- WandB will log metrics if `USE_WANDB=True` and `key.txt` contains a valid key.\n",
    "- I kept the implementation compact and readable; feel free to split cells or extract modules into `.py` files.\n",
    "- You asked me not to run tests; I only modified the notebook in-place. Run training locally when ready."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
