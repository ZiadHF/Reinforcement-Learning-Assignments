{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14321b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\abdul\\_netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "WandB login successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from typing import Tuple, List, Deque\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import wandb\n",
    "\n",
    "# --- 1. Introduction and Setup ---\n",
    "\n",
    "# This notebook implements the DQN and DDQN algorithms based on a more traditional,\n",
    "# function-oriented style inspired by PyTorch tutorials. It is adapted to fulfill\n",
    "# the assignment requirements, including training on multiple environments and\n",
    "# running comprehensive evaluations.\n",
    "\n",
    "# --- Device Detection ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# --- WandB Configuration (Optional) ---\n",
    "# Set USE_WANDB to False if you do not want to log results.\n",
    "USE_WANDB = True\n",
    "PROJECT_NAME = \"RL-Assignment2-Handmade\"\n",
    "\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        # Fix for Windows environments to prevent symlink errors\n",
    "        os.environ['WANDB_CONSOLE'] = 'off'\n",
    "        \n",
    "        # Read API key from a local file\n",
    "        with open(\"key.txt\", \"r\") as f:\n",
    "            api_key = f.read().strip()\n",
    "        wandb.login(key=api_key)\n",
    "        print(\"WandB login successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not log in to WandB: {e}\")\n",
    "        USE_WANDB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9a60b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Hyperparameters and Configuration\n",
    "\n",
    "This cell centralizes all tunable parameters for the experiments. The assignment requires testing different values for these to find the best setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4529619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the AdamW optimizer\n",
    "config = {\n",
    "    \"USE_DDQN\": True,\n",
    "    \"BATCH_SIZE\": 256,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"EPS_START\": 0.9,\n",
    "    \"EPS_END\": 0.05,\n",
    "    \"EPS_DECAY\": 10000,\n",
    "    \"TAU\": 0.005,\n",
    "    \"LR\": 5e-4,\n",
    "    \"REPLAY_MEMORY_SIZE\": 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965beae",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Core DQN Components\n",
    "\n",
    "This section defines the three fundamental building blocks of the agent:\n",
    "1.  **DQN Model**: A simple feed-forward neural network that estimates Q-values.\n",
    "2.  **Replay Memory**: A buffer that stores past experiences (`state`, `action`, `reward`, `next_state`) so the agent can learn from them in batches. This decorrelates experiences and stabilizes training.\n",
    "3.  **Transition**: A `namedtuple` for conveniently storing a single experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f176eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1694069",
   "metadata": {},
   "source": [
    "---\n",
    "### Discretization Wrapper for Continuous Environments\n",
    "\n",
    "Since our DQN agent can only output discrete actions (e.g., action 0, 1, 2), it cannot directly handle environments with continuous action spaces like `Pendulum-v1`.\n",
    "\n",
    "To solve this, we create a **wrapper** class. This class sits \"on top\" of the original environment and modifies its behavior. The `DiscretizeActionWrapper` does the following:\n",
    "1.  It takes the continuous action space (e.g., a range from -2.0 to 2.0 for Pendulum) and converts it into a fixed number of discrete actions (e.g., 5 bins).\n",
    "2.  It tells our agent that there are now 5 possible actions.\n",
    "3.  When our agent picks a discrete action (e.g., action `2`), the wrapper translates it back into the corresponding continuous value (e.g., `0.0`) before passing it to the actual environment.\n",
    "\n",
    "This allows us to use our DQN agent on `Pendulum-v1` without changing the agent's core logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfb2b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizeActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper to discretize a continuous action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_bins):\n",
    "        super().__init__(env)\n",
    "        self.n_bins = n_bins\n",
    "        self.action_space = gym.spaces.Discrete(n_bins)\n",
    "        self.continuous_action_space = env.action_space\n",
    "        \n",
    "        # Create a mapping from discrete actions to continuous values\n",
    "        self.action_map = np.linspace(\n",
    "            self.continuous_action_space.low[0],\n",
    "            self.continuous_action_space.high[0],\n",
    "            n_bins\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the discrete action to a continuous action\n",
    "        continuous_action = np.array([self.action_map[action]], dtype=np.float32)\n",
    "        return self.env.step(continuous_action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921d9a3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training and Optimization Functions\n",
    "\n",
    "This section contains the core logic for the agent's learning process.\n",
    "\n",
    "- **`select_action`**: Implements an epsilon-greedy policy. With probability `epsilon`, it takes a random action (exploration). Otherwise, it takes the action with the highest predicted Q-value (exploitation).\n",
    "- **`optimize_model`**: This is the heart of the learning algorithm. It samples a batch of experiences from the replay memory and computes the loss. It supports both standard DQN and Double DQN (DDQN) based on the `USE_DDQN` flag in the config.\n",
    "- **`get_env_details`**: A helper to get the action and observation space sizes from an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1377ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state, env, policy_net, n_actions):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = config[\"EPS_END\"] + (config[\"EPS_START\"] - config[\"EPS_END\"]) * \\\n",
    "        math.exp(-1. * steps_done / config[\"EPS_DECAY\"])\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_model(memory, policy_net, target_net, optimizer):\n",
    "    if len(memory) < config[\"BATCH_SIZE\"]:\n",
    "        return\n",
    "    transitions = memory.sample(config[\"BATCH_SIZE\"])\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(config[\"BATCH_SIZE\"], device=device)\n",
    "    with torch.no_grad():\n",
    "        if config[\"USE_DDQN\"]:\n",
    "            # DDQN: Use policy_net to select actions, and target_net to evaluate them\n",
    "            best_actions = policy_net(non_final_next_states).argmax(1).unsqueeze(-1)\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, best_actions).squeeze()\n",
    "        else:\n",
    "            # Standard DQN: Use target_net for both selection and evaluation\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "            \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * config[\"GAMMA\"]) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be86d49",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Main Training and Evaluation Loop\n",
    "\n",
    "This is the final, runnable part of the notebook. It automates the entire process as required by the assignment:\n",
    "1.  It defines the list of environments to be trained on.\n",
    "2.  It loops through both **DQN and DDQN** algorithms.\n",
    "3.  For each algorithm and each environment, it:\n",
    "    *   Initializes the policy and target networks, optimizer, and replay memory.\n",
    "    *   Runs the main training loop for a set number of episodes.\n",
    "    *   Logs metrics to WandB (if enabled).\n",
    "    *   Updates the target network periodically.\n",
    "    *   Saves the final trained model.\n",
    "4.  After training, it runs a **100-episode evaluation** to test the agent's performance and prints the average reward.\n",
    "5.  Finally, it prints a summary of all results.\n",
    "\n",
    "**This is the only cell you need to run to complete the assignment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "defb3d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Running Experiment with: DQN\n",
      "========================================\n",
      "\n",
      "--- Training DQN on CartPole-v1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\wandb\\run-20251112_152952-cwhj1k1e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/cwhj1k1e' target=\"_blank\">DQN_CartPole-v1_20251112-152952</a></strong> to <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/cwhj1k1e' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/cwhj1k1e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 0/600 | Reward: 14.0\n",
      "  Episode 50/600 | Reward: 54.0\n",
      "  Episode 100/600 | Reward: 27.0\n",
      "  Episode 150/600 | Reward: 15.0\n",
      "  Episode 200/600 | Reward: 33.0\n",
      "  Episode 250/600 | Reward: 15.0\n",
      "  Episode 300/600 | Reward: 13.0\n",
      "  Episode 350/600 | Reward: 123.0\n",
      "  Episode 400/600 | Reward: 43.0\n",
      "  Episode 450/600 | Reward: 94.0\n",
      "  Episode 500/600 | Reward: 427.0\n",
      "  Episode 550/600 | Reward: 169.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete ---\n",
      "Model saved to DQN_CartPole-v1_policy.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁▁▁▁▁▂▄▃▄▃▅▅▄▂▄▂▃▃▁▆▄▂▅▅▄▇▇▃█▃▁▃▃▃▁▄▄▆▆▁</td></tr><tr><td>reward</td><td>▁▁▁▁▃▁▁▂▁▁▁▂▁▂▁▁▁▁▃▃▁▂▁▄▂▆▄▃▄▃▅▄▄▂▂█▄▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.35333</td></tr><tr><td>reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_CartPole-v1_20251112-152952</strong> at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/cwhj1k1e' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/cwhj1k1e</a><br> View project at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_152952-cwhj1k1e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating DQN on CartPole-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\videos\\DQN_CartPole-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: 497.89\n",
      "Videos saved in: ./videos/DQN_CartPole-v1/\n",
      "\n",
      "--- Training DQN on Acrobot-v1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\wandb\\run-20251112_153526-irleml8e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/irleml8e' target=\"_blank\">DQN_Acrobot-v1_20251112-153526</a></strong> to <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/irleml8e' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/irleml8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 0/600 | Reward: -500.0\n",
      "  Episode 50/600 | Reward: -151.0\n",
      "  Episode 100/600 | Reward: -85.0\n",
      "  Episode 150/600 | Reward: -99.0\n",
      "  Episode 200/600 | Reward: -122.0\n",
      "  Episode 250/600 | Reward: -105.0\n",
      "  Episode 300/600 | Reward: -79.0\n",
      "  Episode 350/600 | Reward: -84.0\n",
      "  Episode 400/600 | Reward: -69.0\n",
      "  Episode 450/600 | Reward: -105.0\n",
      "  Episode 500/600 | Reward: -137.0\n",
      "  Episode 550/600 | Reward: -89.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete ---\n",
      "Model saved to DQN_Acrobot-v1_policy.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▂▁▂▄▃▃▃▃▄▂▄▂▄▃▄▃▃▃▄▃▅▄▃▃▃█▃▅▅▃▆▄▃▄▂▄▂▂▄▅</td></tr><tr><td>reward</td><td>▁▅▇▇█▇▇██▇█▇▇▇▇▇█▇▇▅▇██▇████▇██▇▇██████▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.08868</td></tr><tr><td>reward</td><td>-125</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_Acrobot-v1_20251112-153526</strong> at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/irleml8e' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/irleml8e</a><br> View project at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_153526-irleml8e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating DQN on Acrobot-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\videos\\DQN_Acrobot-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -83.49\n",
      "Videos saved in: ./videos/DQN_Acrobot-v1/\n",
      "\n",
      "--- Found pre-trained model for DQN on MountainCar-v0 ---\n",
      "Loading model from DQN_MountainCar-v0_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DQN on MountainCar-v0 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Temp\\ipykernel_14588\\1507937237.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(model_path))\n",
      "c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\videos\\DQN_MountainCar-v0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -124.80\n",
      "Videos saved in: ./videos/DQN_MountainCar-v0/\n",
      "\n",
      "--- Detected continuous action space for Pendulum-v1. Applying discretization wrapper. ---\n",
      "--- Training DQN on Pendulum-v1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\wandb\\run-20251112_154040-iud0kwyf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/iud0kwyf' target=\"_blank\">DQN_Pendulum-v1_20251112-154040</a></strong> to <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/iud0kwyf' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/iud0kwyf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 0/600 | Reward: -1230.128544008052\n",
      "  Episode 50/600 | Reward: -594.5730136648375\n",
      "  Episode 100/600 | Reward: -261.0254768473969\n",
      "  Episode 150/600 | Reward: -129.34940810035948\n",
      "  Episode 200/600 | Reward: -125.99851514303865\n",
      "  Episode 250/600 | Reward: -125.5095578590799\n",
      "  Episode 300/600 | Reward: -130.579834315916\n",
      "  Episode 350/600 | Reward: -237.5144858494419\n",
      "  Episode 400/600 | Reward: -1.8524709709371339\n",
      "  Episode 450/600 | Reward: -120.9815546926162\n",
      "  Episode 500/600 | Reward: -1.7957169270728772\n",
      "  Episode 550/600 | Reward: -116.88051830032457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete ---\n",
      "Model saved to DQN_Pendulum-v1_policy.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▇▆█▄▄▄▂▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▃▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>▁▅▄▅▇▆▅▆▆▆▆▅▇▇▇█▇▇▆▇▇▇▇▇▇▇▇█▇▆██▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.04737</td></tr><tr><td>reward</td><td>-571.5583</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_Pendulum-v1_20251112-154040</strong> at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/iud0kwyf' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/iud0kwyf</a><br> View project at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_154040-iud0kwyf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating DQN on Pendulum-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\videos\\DQN_Pendulum-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -308.99\n",
      "Videos saved in: ./videos/DQN_Pendulum-v1/\n",
      "\n",
      "\n",
      "========================================\n",
      "Running Experiment with: DDQN\n",
      "========================================\n",
      "\n",
      "--- Training DDQN on CartPole-v1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\wandb\\run-20251112_154939-xf6vzbbj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/xf6vzbbj' target=\"_blank\">DDQN_CartPole-v1_20251112-154939</a></strong> to <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/xf6vzbbj' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/xf6vzbbj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 0/600 | Reward: 19.0\n",
      "  Episode 50/600 | Reward: 48.0\n",
      "  Episode 100/600 | Reward: 47.0\n",
      "  Episode 150/600 | Reward: 14.0\n",
      "  Episode 200/600 | Reward: 81.0\n",
      "  Episode 250/600 | Reward: 90.0\n",
      "  Episode 300/600 | Reward: 22.0\n",
      "  Episode 350/600 | Reward: 23.0\n",
      "  Episode 400/600 | Reward: 141.0\n",
      "  Episode 450/600 | Reward: 103.0\n",
      "  Episode 500/600 | Reward: 87.0\n",
      "  Episode 550/600 | Reward: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete ---\n",
      "Model saved to DDQN_CartPole-v1_policy.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁▁▁▁▁▂▁▂▂▂▂▁▂▂▂▂▃▂▁▂▁▅▂▃▂▄▃▄▂▁▃▃▃▅▄▄▂██▁</td></tr><tr><td>reward</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▃▁▁▄▂▅▁▁▄▃▆▂█▃▃▃▃▃▃▃▂▄▄▂▄▅▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.00179</td></tr><tr><td>reward</td><td>49</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DDQN_CartPole-v1_20251112-154939</strong> at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/xf6vzbbj' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/xf6vzbbj</a><br> View project at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_154939-xf6vzbbj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating DDQN on CartPole-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\videos\\DDQN_CartPole-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: 452.83\n",
      "Videos saved in: ./videos/DDQN_CartPole-v1/\n",
      "\n",
      "--- Training DDQN on Acrobot-v1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\wandb\\run-20251112_155544-wp8tcure</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/wp8tcure' target=\"_blank\">DDQN_Acrobot-v1_20251112-155544</a></strong> to <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/wp8tcure' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/wp8tcure</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 0/600 | Reward: -500.0\n",
      "  Episode 50/600 | Reward: -182.0\n",
      "  Episode 100/600 | Reward: -109.0\n",
      "  Episode 150/600 | Reward: -126.0\n",
      "  Episode 200/600 | Reward: -116.0\n",
      "  Episode 250/600 | Reward: -90.0\n",
      "  Episode 300/600 | Reward: -72.0\n",
      "  Episode 350/600 | Reward: -105.0\n",
      "  Episode 400/600 | Reward: -79.0\n",
      "  Episode 450/600 | Reward: -85.0\n",
      "  Episode 500/600 | Reward: -83.0\n",
      "  Episode 550/600 | Reward: -133.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete ---\n",
      "Model saved to DDQN_Acrobot-v1_policy.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▇█▃▂▂▂▁▂▁▁▃▃▃▂▂▂▃▅▂▃▃▂▂▅▃▂▃▅▃▂▂▁▃▃▃▄▃▂▃▂</td></tr><tr><td>reward</td><td>▁▅▄▇▇▇█▇▇█▇█▇▇██▇▇▇▇█▇▇▇▇████▇▆██▇▇▇▇█▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.15766</td></tr><tr><td>reward</td><td>-89</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DDQN_Acrobot-v1_20251112-155544</strong> at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/wp8tcure' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/wp8tcure</a><br> View project at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_155544-wp8tcure\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating DDQN on Acrobot-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\videos\\DDQN_Acrobot-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -77.65\n",
      "Videos saved in: ./videos/DDQN_Acrobot-v1/\n",
      "\n",
      "--- Found pre-trained model for DDQN on MountainCar-v0 ---\n",
      "Loading model from DDQN_MountainCar-v0_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DDQN on MountainCar-v0 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Temp\\ipykernel_14588\\1507937237.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(model_path))\n",
      "c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\videos\\DDQN_MountainCar-v0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -100.54\n",
      "Videos saved in: ./videos/DDQN_MountainCar-v0/\n",
      "\n",
      "--- Detected continuous action space for Pendulum-v1. Applying discretization wrapper. ---\n",
      "--- Training DDQN on Pendulum-v1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\wandb\\run-20251112_160122-hohveys0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/hohveys0' target=\"_blank\">DDQN_Pendulum-v1_20251112-160122</a></strong> to <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/hohveys0' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/hohveys0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 0/600 | Reward: -783.7028029846276\n",
      "  Episode 50/600 | Reward: -587.2922603404882\n",
      "  Episode 100/600 | Reward: -484.26471836397855\n",
      "  Episode 150/600 | Reward: -230.2755994181756\n",
      "  Episode 200/600 | Reward: -121.0052603403164\n",
      "  Episode 250/600 | Reward: -126.4202810095579\n",
      "  Episode 300/600 | Reward: -0.8303889588935477\n",
      "  Episode 350/600 | Reward: -236.21510642857348\n",
      "  Episode 400/600 | Reward: -9.248515777936753\n",
      "  Episode 450/600 | Reward: -247.95161115476023\n",
      "  Episode 500/600 | Reward: -122.5326721414868\n",
      "  Episode 550/600 | Reward: -292.3699343008512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete ---\n",
      "Model saved to DDQN_Pendulum-v1_policy.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁▁▁▄▆█▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>▁▄▃▂▅▃▇▆▇▆▇▇▅██▆█▇█▇▇▇▇▇▇▇▆▇▇▇▇▆▇▇█▇▇█▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.02677</td></tr><tr><td>reward</td><td>-6.29557</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DDQN_Pendulum-v1_20251112-160122</strong> at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/hohveys0' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade/runs/hohveys0</a><br> View project at: <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Handmade</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_160122-hohveys0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating DDQN on Pendulum-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\videos\\DDQN_Pendulum-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -141.92\n",
      "Videos saved in: ./videos/DDQN_Pendulum-v1/\n",
      "\n",
      "\n",
      "\n",
      "--- Overall Evaluation Summary ---\n",
      "\n",
      "--- DQN Results ---\n",
      "  Environment: CartPole-v1 | Average Reward (100 eps): 497.89\n",
      "  Environment: Acrobot-v1 | Average Reward (100 eps): -83.49\n",
      "  Environment: MountainCar-v0 | Average Reward (100 eps): -124.80\n",
      "  Environment: Pendulum-v1 | Average Reward (100 eps): -308.99\n",
      "\n",
      "--- DDQN Results ---\n",
      "  Environment: CartPole-v1 | Average Reward (100 eps): 452.83\n",
      "  Environment: Acrobot-v1 | Average Reward (100 eps): -77.65\n",
      "  Environment: MountainCar-v0 | Average Reward (100 eps): -100.54\n",
      "  Environment: Pendulum-v1 | Average Reward (100 eps): -141.92\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Environments to run ---\n",
    "# \"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\", \"Pendulum-v1\"\n",
    "environments = [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\", \"Pendulum-v1\"]\n",
    "evaluation_results = {}\n",
    "\n",
    "# --- Main Experiment Loop ---\n",
    "for use_ddqn_flag in [False, True]:\n",
    "    algo_name = \"DDQN\" if use_ddqn_flag else \"DQN\"\n",
    "    config[\"USE_DDQN\"] = use_ddqn_flag\n",
    "    print(f\"\\n{'='*40}\\nRunning Experiment with: {algo_name}\\n{'='*40}\\n\")\n",
    "    \n",
    "    if algo_name not in evaluation_results:\n",
    "        evaluation_results[algo_name] = {}\n",
    "\n",
    "    for env_name in environments:\n",
    "        model_path = f\"{algo_name}_{env_name}_policy.pth\"\n",
    "\n",
    "        # --- Environment Setup ---\n",
    "        is_continuous = False\n",
    "        temp_env = gym.make(env_name)\n",
    "        if isinstance(temp_env.action_space, gym.spaces.Box):\n",
    "            is_continuous = True\n",
    "        temp_env.close()\n",
    "\n",
    "        if is_continuous:\n",
    "            print(f\"--- Detected continuous action space for {env_name}. Applying discretization wrapper. ---\")\n",
    "            # For Pendulum, we discretize the action space into 11 bins\n",
    "            env = DiscretizeActionWrapper(gym.make(env_name), n_bins=5)\n",
    "        else:\n",
    "            env = gym.make(env_name)\n",
    "            \n",
    "        # Get action and observation space sizes\n",
    "        n_actions = env.action_space.n\n",
    "        state, _ = env.reset()\n",
    "        n_observations = len(state)\n",
    "        env.reset() # Reset again to ensure clean state for training\n",
    "\n",
    "        policy_net = DQN(n_observations, n_actions).to(device)\n",
    "        \n",
    "        # --- Check if model is already trained ---\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"--- Found pre-trained model for {algo_name} on {env_name} ---\")\n",
    "            print(f\"Loading model from {model_path} and skipping training.\\n\")\n",
    "            policy_net.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            print(f\"--- Training {algo_name} on {env_name} ---\")\n",
    "            target_net = DQN(n_observations, n_actions).to(device)\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            optimizer = optim.AdamW(policy_net.parameters(), lr=config[\"LR\"], amsgrad=True)\n",
    "            memory = ReplayMemory(config[\"REPLAY_MEMORY_SIZE\"])\n",
    "            \n",
    "            steps_done = 0\n",
    "            \n",
    "            # --- WandB Setup ---\n",
    "            run_name = f\"{algo_name}_{env_name}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "            if USE_WANDB:\n",
    "                if wandb.run is not None: wandb.finish()\n",
    "                wandb.init(project=PROJECT_NAME, name=run_name, config=config, reinit=True)\n",
    "\n",
    "            # --- Training Loop ---\n",
    "            num_episodes = 1500 if env_name == \"MountainCar-v0\" else 600\n",
    "            for i_episode in range(num_episodes):\n",
    "                state, info = env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                ep_reward = 0\n",
    "                \n",
    "                for t in range(1000): # Max steps per episode\n",
    "                    action = select_action(state, env, policy_net, n_actions)\n",
    "                    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                    ep_reward += reward\n",
    "                    reward = torch.tensor([reward], device=device)\n",
    "                    done = terminated or truncated\n",
    "\n",
    "                    if terminated:\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                    memory.push(state, action, next_state, reward)\n",
    "                    state = next_state\n",
    "\n",
    "                    loss = optimize_model(memory, policy_net, target_net, optimizer)\n",
    "                    \n",
    "                    target_net_state_dict = target_net.state_dict()\n",
    "                    policy_net_state_dict = policy_net.state_dict()\n",
    "                    for key in policy_net_state_dict:\n",
    "                        target_net_state_dict[key] = policy_net_state_dict[key]*config[\"TAU\"] + target_net_state_dict[key]*(1-config[\"TAU\"])\n",
    "                    target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                if USE_WANDB and loss is not None:\n",
    "                    wandb.log({\"reward\": ep_reward, \"loss\": loss}, step=i_episode)\n",
    "                \n",
    "                if i_episode % 50 == 0:\n",
    "                    print(f\"  Episode {i_episode}/{num_episodes} | Reward: {ep_reward}\")\n",
    "\n",
    "            print(\"--- Training Complete ---\")\n",
    "            torch.save(policy_net.state_dict(), model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "            if USE_WANDB: wandb.finish()\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        print(f\"--- Evaluating {algo_name} on {env_name} for 100 episodes (with video recording) ---\")\n",
    "        \n",
    "        # Create a directory for videos if it doesn't exist\n",
    "        video_folder = f\"./videos/{algo_name}_{env_name}/\"\n",
    "        os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "        # Setup evaluation environment (with discretization if needed)\n",
    "        if is_continuous:\n",
    "            eval_env = DiscretizeActionWrapper(gym.make(env_name, render_mode=\"rgb_array\"), n_bins=5)\n",
    "        else:\n",
    "            eval_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        \n",
    "        # Wrap with video recorder\n",
    "        eval_env = gym.wrappers.RecordVideo(\n",
    "            eval_env, \n",
    "            video_folder,\n",
    "            episode_trigger=lambda x: x % 25 == 0, # Record every 25 episodes\n",
    "            name_prefix=f\"{algo_name}-{env_name}\"\n",
    "        )\n",
    "\n",
    "        total_eval_reward = 0\n",
    "        for i in range(100):\n",
    "            state, _ = eval_env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            ep_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(state).max(1)[1].view(1, 1)\n",
    "                obs, reward, terminated, truncated, _ = eval_env.step(action.item())\n",
    "                ep_reward += reward\n",
    "                done = terminated or truncated\n",
    "                if not done:\n",
    "                    state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            total_eval_reward += ep_reward\n",
    "        \n",
    "        avg_reward = total_eval_reward / 100\n",
    "        evaluation_results[algo_name][env_name] = avg_reward\n",
    "        print(f\"Average Reward over 100 episodes: {avg_reward:.2f}\")\n",
    "        print(f\"Videos saved in: {video_folder}\\n\")\n",
    "        \n",
    "        eval_env.close()\n",
    "        env.close()\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\\n--- Overall Evaluation Summary ---\")\n",
    "for algo_name, results in evaluation_results.items():\n",
    "    print(f\"\\n--- {algo_name} Results ---\")\n",
    "    for env_name, avg_reward in results.items():\n",
    "        print(f\"  Environment: {env_name} | Average Reward (100 eps): {avg_reward:.2f}\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62552c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
