{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14321b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Ziad\\_netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB login successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from typing import Tuple, List, Deque\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import wandb\n",
    "\n",
    "# --- 1. Introduction and Setup ---\n",
    "\n",
    "# This notebook implements the DQN and DDQN algorithms based on a more traditional,\n",
    "# function-oriented style inspired by PyTorch tutorials. It is adapted to fulfill\n",
    "# the assignment requirements, including training on multiple environments and\n",
    "# running comprehensive evaluations.\n",
    "\n",
    "# --- Device Detection ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# --- WandB Configuration (Optional) ---\n",
    "# Set USE_WANDB to False if you do not want to log results.\n",
    "USE_WANDB = True\n",
    "PROJECT_NAME = \"RL-Assignment2-EVALS1\"\n",
    "\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        # Fix for Windows environments to prevent symlink errors\n",
    "        os.environ['WANDB_CONSOLE'] = 'off'\n",
    "        \n",
    "        # Read API key from a local file\n",
    "        with open(\"key.txt\", \"r\") as f:\n",
    "            api_key = f.read().strip()\n",
    "        wandb.login(key=api_key)\n",
    "        print(\"WandB login successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not log in to WandB: {e}\")\n",
    "        USE_WANDB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9a60b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Hyperparameters and Configuration\n",
    "\n",
    "This cell centralizes all tunable parameters for the experiments. The assignment requires testing different values for these to find the best setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4529619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the AdamW optimizer\n",
    "# REPLAY_MEMORY_SIZE is the maximum size of the replay buffer\n",
    "\n",
    "# --- Default Configuration ---\n",
    "# These are baseline values. Environment-specific configs below will override them.\n",
    "default_config = {\n",
    "    \"USE_DDQN\": True,\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"EPS_START\": 0.9,\n",
    "    \"EPS_END\": 0.05,\n",
    "    \"EPS_DECAY\": 1000,\n",
    "    \"TAU\": 0.005,\n",
    "    \"LR\": 5e-4,\n",
    "    \"REPLAY_MEMORY_SIZE\": 10000,\n",
    "    \"NUM_EPISODES\": 600,\n",
    "    \"TARGET_UPDATE\": 10,  # Update target network every N steps\n",
    "    \"N_BINS\": 16 # Default discretization bins for continuous envs\n",
    "}\n",
    "\n",
    "# --- Environment-Specific Configurations ---\n",
    "# Here, we override the default config for each environment to optimize performance.\n",
    "env_configs = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"BATCH_SIZE\": 64,\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"EPS_START\": 0.9,\n",
    "        \"EPS_END\": 0.05,\n",
    "        \"EPS_DECAY\": 200,\n",
    "        \"TAU\": 0.005,\n",
    "        \"LR\": 1e-3,\n",
    "        \"REPLAY_MEMORY_SIZE\": 10000,\n",
    "        \"NUM_EPISODES\": 600,\n",
    "        \"TARGET_UPDATE\": 4\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"BATCH_SIZE\": 128,\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"EPS_START\": 1.0,\n",
    "        \"EPS_END\": 0.01,\n",
    "        \"EPS_DECAY\": 1000,\n",
    "        \"TAU\": 0.005,\n",
    "        \"LR\": 5e-4,\n",
    "        \"REPLAY_MEMORY_SIZE\": 50000,\n",
    "        \"NUM_EPISODES\": 1000,\n",
    "        \"TARGET_UPDATE\": 10\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"BATCH_SIZE\": 128,\n",
    "        \"GAMMA\": 0.999,  # Higher discount for sparse rewards\n",
    "        \"EPS_START\": 1.0,\n",
    "        \"EPS_END\": 0.01,\n",
    "        \"EPS_DECAY\": 2000,\n",
    "        \"TAU\": 0.005,\n",
    "        \"LR\": 1e-3,\n",
    "        \"REPLAY_MEMORY_SIZE\": 50000,\n",
    "        \"NUM_EPISODES\": 1500,\n",
    "        \"TARGET_UPDATE\": 10\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"BATCH_SIZE\": 256,\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"EPS_START\": 1.0,\n",
    "        \"EPS_END\": 0.05,\n",
    "        \"EPS_DECAY\": 500,\n",
    "        \"TAU\": 0.005,\n",
    "        \"LR\": 5e-4,\n",
    "        \"REPLAY_MEMORY_SIZE\": 100000,\n",
    "        \"NUM_EPISODES\": 500,\n",
    "        \"TARGET_UPDATE\": 10,\n",
    "        \"N_BINS\": 5  # More actions for smoother control\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965beae",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Core DQN Components\n",
    "\n",
    "This section defines the three fundamental building blocks of the agent:\n",
    "1.  **DQN Model**: A simple feed-forward neural network that estimates Q-values.\n",
    "2.  **Replay Memory**: A buffer that stores past experiences (`state`, `action`, `reward`, `next_state`) so the agent can learn from them in batches. This decorrelates experiences and stabilizes training.\n",
    "3.  **Transition**: A `namedtuple` for conveniently storing a single experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f176eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1694069",
   "metadata": {},
   "source": [
    "---\n",
    "### Discretization Wrapper for Continuous Environments\n",
    "\n",
    "Since our DQN agent can only output discrete actions (e.g., action 0, 1, 2), it cannot directly handle environments with continuous action spaces like `Pendulum-v1`.\n",
    "\n",
    "To solve this, we create a **wrapper** class. This class sits \"on top\" of the original environment and modifies its behavior. The `DiscretizeActionWrapper` does the following:\n",
    "1.  It takes the continuous action space (e.g., a range from -2.0 to 2.0 for Pendulum) and converts it into a fixed number of discrete actions (e.g., 5 bins).\n",
    "2.  It tells our agent that there are now 5 possible actions.\n",
    "3.  When our agent picks a discrete action (e.g., action `2`), the wrapper translates it back into the corresponding continuous value (e.g., `0.0`) before passing it to the actual environment.\n",
    "\n",
    "This allows us to use our DQN agent on `Pendulum-v1` without changing the agent's core logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dfb2b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizeActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper to discretize a continuous action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_bins):\n",
    "        super().__init__(env)\n",
    "        self.n_bins = n_bins\n",
    "        self.action_space = gym.spaces.Discrete(n_bins)\n",
    "        self.continuous_action_space = env.action_space\n",
    "        \n",
    "        # Create a mapping from discrete actions to continuous values\n",
    "        self.action_map = np.linspace(\n",
    "            self.continuous_action_space.low[0],\n",
    "            self.continuous_action_space.high[0],\n",
    "            n_bins\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the discrete action to a continuous action\n",
    "        continuous_action = np.array([self.action_map[action]], dtype=np.float32)\n",
    "        return self.env.step(continuous_action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921d9a3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training and Optimization Functions\n",
    "\n",
    "This section contains the core logic for the agent's learning process.\n",
    "\n",
    "- **`select_action`**: Implements an epsilon-greedy policy. With probability `epsilon`, it takes a random action (exploration). Otherwise, it takes the action with the highest predicted Q-value (exploitation).\n",
    "- **`optimize_model`**: This is the heart of the learning algorithm. It samples a batch of experiences from the replay memory and computes the loss. It supports both standard DQN and Double DQN (DDQN) based on the `USE_DDQN` flag in the config.\n",
    "- **`get_env_details`**: A helper to get the action and observation space sizes from an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1377ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, env, policy_net, n_actions, config, steps_done):\n",
    "    \"\"\"\n",
    "    Select action using epsilon-greedy policy.\n",
    "    Returns: action tensor, updated steps_done\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    eps_threshold = config[\"EPS_END\"] + (config[\"EPS_START\"] - config[\"EPS_END\"]) * \\\n",
    "        math.exp(-1. * steps_done / config[\"EPS_DECAY\"])\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1), steps_done\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long), steps_done\n",
    "\n",
    "def optimize_model(memory, policy_net, target_net, optimizer, config):\n",
    "    if len(memory) < config[\"BATCH_SIZE\"]:\n",
    "        return None\n",
    "    transitions = memory.sample(config[\"BATCH_SIZE\"])\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(config[\"BATCH_SIZE\"], device=device)\n",
    "    with torch.no_grad():\n",
    "        if config[\"USE_DDQN\"]:\n",
    "            # DDQN: Use policy_net to select actions, and target_net to evaluate them\n",
    "            best_actions = policy_net(non_final_next_states).argmax(1).unsqueeze(-1)\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, best_actions).squeeze()\n",
    "        else:\n",
    "            # Standard DQN: Use target_net for both selection and evaluation\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "            \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * config[\"GAMMA\"]) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be86d49",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Main Training and Evaluation Loop\n",
    "\n",
    "This is the final, runnable part of the notebook. It automates the entire process as required by the assignment:\n",
    "1.  It defines the list of environments to be trained on.\n",
    "2.  It loops through both **DQN and DDQN** algorithms.\n",
    "3.  For each algorithm and each environment, it:\n",
    "    *   Initializes the policy and target networks, optimizer, and replay memory.\n",
    "    *   Runs the main training loop for a set number of episodes.\n",
    "    *   Logs metrics to WandB (if enabled).\n",
    "    *   Updates the target network periodically.\n",
    "    *   Saves the final trained model.\n",
    "4.  After training, it runs a **100-episode evaluation** to test the agent's performance and prints the average reward.\n",
    "5.  Finally, it prints a summary of all results.\n",
    "\n",
    "**This is the only cell you need to run to complete the assignment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "defb3d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Running Experiment with: DQN\n",
      "========================================\n",
      "\n",
      "--- Found pre-trained model for DQN on CartPole-v1 ---\n",
      "Loading model from DQN_CartPole-v1_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DQN on CartPole-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: WARN: Overwriting existing videos at d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\videos\\DQN_CartPole-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: 500.00\n",
      "Videos saved in: ./videos/DQN_CartPole-v1/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\wandb\\run-20251112_213706-olqko4g3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/olqko4g3' target=\"_blank\">DQN_CartPole-v1_20251112-213641_eval</a></strong> to <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/olqko4g3' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/olqko4g3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>▁</td></tr><tr><td>evaluation/avg_reward</td><td>▁</td></tr><tr><td>evaluation/max_reward</td><td>▁</td></tr><tr><td>evaluation/min_reward</td><td>▁</td></tr><tr><td>evaluation/std_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>500</td></tr><tr><td>evaluation/avg_reward</td><td>500</td></tr><tr><td>evaluation/max_reward</td><td>500</td></tr><tr><td>evaluation/min_reward</td><td>500</td></tr><tr><td>evaluation/std_reward</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_CartPole-v1_20251112-213641_eval</strong> at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/olqko4g3' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/olqko4g3</a><br> View project at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_213706-olqko4g3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Found pre-trained model for DQN on Acrobot-v1 ---\n",
      "Loading model from DQN_Acrobot-v1_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DQN on Acrobot-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: WARN: Overwriting existing videos at d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\videos\\DQN_Acrobot-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -78.13\n",
      "Videos saved in: ./videos/DQN_Acrobot-v1/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\wandb\\run-20251112_213717-y5jctdym</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/y5jctdym' target=\"_blank\">DQN_Acrobot-v1_20251112-213710_eval</a></strong> to <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/y5jctdym' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/y5jctdym</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>▁</td></tr><tr><td>evaluation/avg_reward</td><td>▁</td></tr><tr><td>evaluation/max_reward</td><td>▁</td></tr><tr><td>evaluation/min_reward</td><td>▁</td></tr><tr><td>evaluation/std_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>79.13</td></tr><tr><td>evaluation/avg_reward</td><td>-78.13</td></tr><tr><td>evaluation/max_reward</td><td>-62</td></tr><tr><td>evaluation/min_reward</td><td>-162</td></tr><tr><td>evaluation/std_reward</td><td>12.13808</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_Acrobot-v1_20251112-213710_eval</strong> at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/y5jctdym' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/y5jctdym</a><br> View project at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_213717-y5jctdym\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Found pre-trained model for DQN on MountainCar-v0 ---\n",
      "Loading model from DQN_MountainCar-v0_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DQN on MountainCar-v0 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: WARN: Overwriting existing videos at d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\videos\\DQN_MountainCar-v0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -126.08\n",
      "Videos saved in: ./videos/DQN_MountainCar-v0/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\wandb\\run-20251112_213729-bq9yk6v4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/bq9yk6v4' target=\"_blank\">DQN_MountainCar-v0_20251112-213721_eval</a></strong> to <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/bq9yk6v4' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/bq9yk6v4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>▁</td></tr><tr><td>evaluation/avg_reward</td><td>▁</td></tr><tr><td>evaluation/max_reward</td><td>▁</td></tr><tr><td>evaluation/min_reward</td><td>▁</td></tr><tr><td>evaluation/std_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>126.08</td></tr><tr><td>evaluation/avg_reward</td><td>-126.08</td></tr><tr><td>evaluation/max_reward</td><td>-85</td></tr><tr><td>evaluation/min_reward</td><td>-200</td></tr><tr><td>evaluation/std_reward</td><td>24.69116</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_MountainCar-v0_20251112-213721_eval</strong> at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/bq9yk6v4' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/bq9yk6v4</a><br> View project at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_213729-bq9yk6v4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Detected continuous action space for Pendulum-v1. Applying discretization wrapper. ---\n",
      "--- Found pre-trained model for DQN on Pendulum-v1 ---\n",
      "Loading model from DQN_Pendulum-v1_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DQN on Pendulum-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: WARN: Overwriting existing videos at d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\videos\\DQN_Pendulum-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -173.47\n",
      "Videos saved in: ./videos/DQN_Pendulum-v1/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\wandb\\run-20251112_213747-w4htlubi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/w4htlubi' target=\"_blank\">DQN_Pendulum-v1_20251112-213734_eval</a></strong> to <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/w4htlubi' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/w4htlubi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>▁</td></tr><tr><td>evaluation/avg_reward</td><td>▁</td></tr><tr><td>evaluation/max_reward</td><td>▁</td></tr><tr><td>evaluation/min_reward</td><td>▁</td></tr><tr><td>evaluation/std_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>200</td></tr><tr><td>evaluation/avg_reward</td><td>-173.47456</td></tr><tr><td>evaluation/max_reward</td><td>-0.40574</td></tr><tr><td>evaluation/min_reward</td><td>-479.55548</td></tr><tr><td>evaluation/std_reward</td><td>90.65366</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_Pendulum-v1_20251112-213734_eval</strong> at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/w4htlubi' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/w4htlubi</a><br> View project at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_213747-w4htlubi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Running Experiment with: DDQN\n",
      "========================================\n",
      "\n",
      "--- Found pre-trained model for DDQN on CartPole-v1 ---\n",
      "Loading model from DDQN_CartPole-v1_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DDQN on CartPole-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: WARN: Overwriting existing videos at d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\videos\\DDQN_CartPole-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: 500.00\n",
      "Videos saved in: ./videos/DDQN_CartPole-v1/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\wandb\\run-20251112_213816-p3nexmwv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/p3nexmwv' target=\"_blank\">DDQN_CartPole-v1_20251112-213752_eval</a></strong> to <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/p3nexmwv' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/p3nexmwv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>▁</td></tr><tr><td>evaluation/avg_reward</td><td>▁</td></tr><tr><td>evaluation/max_reward</td><td>▁</td></tr><tr><td>evaluation/min_reward</td><td>▁</td></tr><tr><td>evaluation/std_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>500</td></tr><tr><td>evaluation/avg_reward</td><td>500</td></tr><tr><td>evaluation/max_reward</td><td>500</td></tr><tr><td>evaluation/min_reward</td><td>500</td></tr><tr><td>evaluation/std_reward</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DDQN_CartPole-v1_20251112-213752_eval</strong> at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/p3nexmwv' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/p3nexmwv</a><br> View project at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_213816-p3nexmwv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Found pre-trained model for DDQN on Acrobot-v1 ---\n",
      "Loading model from DDQN_Acrobot-v1_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DDQN on Acrobot-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: WARN: Overwriting existing videos at d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\videos\\DDQN_Acrobot-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -67.52\n",
      "Videos saved in: ./videos/DDQN_Acrobot-v1/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\wandb\\run-20251112_213827-fzjikjt5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/fzjikjt5' target=\"_blank\">DDQN_Acrobot-v1_20251112-213822_eval</a></strong> to <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/fzjikjt5' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/fzjikjt5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>▁</td></tr><tr><td>evaluation/avg_reward</td><td>▁</td></tr><tr><td>evaluation/max_reward</td><td>▁</td></tr><tr><td>evaluation/min_reward</td><td>▁</td></tr><tr><td>evaluation/std_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>68.52</td></tr><tr><td>evaluation/avg_reward</td><td>-67.52</td></tr><tr><td>evaluation/max_reward</td><td>-61</td></tr><tr><td>evaluation/min_reward</td><td>-137</td></tr><tr><td>evaluation/std_reward</td><td>9.8919</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DDQN_Acrobot-v1_20251112-213822_eval</strong> at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/fzjikjt5' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/fzjikjt5</a><br> View project at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_213827-fzjikjt5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Found pre-trained model for DDQN on MountainCar-v0 ---\n",
      "Loading model from DDQN_MountainCar-v0_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DDQN on MountainCar-v0 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: WARN: Overwriting existing videos at d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\videos\\DDQN_MountainCar-v0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -100.60\n",
      "Videos saved in: ./videos/DDQN_MountainCar-v0/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\wandb\\run-20251112_213839-lfghhbh4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/lfghhbh4' target=\"_blank\">DDQN_MountainCar-v0_20251112-213832_eval</a></strong> to <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/lfghhbh4' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/lfghhbh4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>▁</td></tr><tr><td>evaluation/avg_reward</td><td>▁</td></tr><tr><td>evaluation/max_reward</td><td>▁</td></tr><tr><td>evaluation/min_reward</td><td>▁</td></tr><tr><td>evaluation/std_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>100.6</td></tr><tr><td>evaluation/avg_reward</td><td>-100.6</td></tr><tr><td>evaluation/max_reward</td><td>-83</td></tr><tr><td>evaluation/min_reward</td><td>-105</td></tr><tr><td>evaluation/std_reward</td><td>7.79615</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DDQN_MountainCar-v0_20251112-213832_eval</strong> at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/lfghhbh4' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/lfghhbh4</a><br> View project at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_213839-lfghhbh4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Detected continuous action space for Pendulum-v1. Applying discretization wrapper. ---\n",
      "--- Found pre-trained model for DDQN on Pendulum-v1 ---\n",
      "Loading model from DDQN_Pendulum-v1_policy.pth and skipping training.\n",
      "\n",
      "--- Evaluating DDQN on Pendulum-v1 for 100 episodes (with video recording) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: WARN: Overwriting existing videos at d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\videos\\DDQN_Pendulum-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: -141.75\n",
      "Videos saved in: ./videos/DDQN_Pendulum-v1/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Program Files (x86)\\Codes\\CMPS458\\Assignment 2\\wandb\\run-20251112_213856-actwbkge</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/actwbkge' target=\"_blank\">DDQN_Pendulum-v1_20251112-213843_eval</a></strong> to <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/actwbkge' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/actwbkge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>▁</td></tr><tr><td>evaluation/avg_reward</td><td>▁</td></tr><tr><td>evaluation/max_reward</td><td>▁</td></tr><tr><td>evaluation/min_reward</td><td>▁</td></tr><tr><td>evaluation/std_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/avg_duration</td><td>200</td></tr><tr><td>evaluation/avg_reward</td><td>-141.75277</td></tr><tr><td>evaluation/max_reward</td><td>-0.49669</td></tr><tr><td>evaluation/min_reward</td><td>-344.58894</td></tr><tr><td>evaluation/std_reward</td><td>78.23031</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DDQN_Pendulum-v1_20251112-213843_eval</strong> at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/actwbkge' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1/runs/actwbkge</a><br> View project at: <a href='https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1' target=\"_blank\">https://wandb.ai/ziadhf-cairo-university/RL-Assignment2-EVALS1</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_213856-actwbkge\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Overall Evaluation Summary ---\n",
      "\n",
      "--- DQN Results ---\n",
      "  Environment: CartPole-v1 | Average Reward (100 eps): 500.00\n",
      "  Environment: Acrobot-v1 | Average Reward (100 eps): -78.13\n",
      "  Environment: MountainCar-v0 | Average Reward (100 eps): -126.08\n",
      "  Environment: Pendulum-v1 | Average Reward (100 eps): -173.47\n",
      "\n",
      "--- DDQN Results ---\n",
      "  Environment: CartPole-v1 | Average Reward (100 eps): 500.00\n",
      "  Environment: Acrobot-v1 | Average Reward (100 eps): -67.52\n",
      "  Environment: MountainCar-v0 | Average Reward (100 eps): -100.60\n",
      "  Environment: Pendulum-v1 | Average Reward (100 eps): -141.75\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Environments to run ---\n",
    "environments = [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\", \"Pendulum-v1\"]\n",
    "evaluation_results = {}\n",
    "\n",
    "# --- Main Experiment Loop ---\n",
    "for use_ddqn_flag in [False, True]:\n",
    "    algo_name = \"DDQN\" if use_ddqn_flag else \"DQN\"\n",
    "    print(f\"\\n{'='*40}\\nRunning Experiment with: {algo_name}\\n{'='*40}\\n\")\n",
    "    \n",
    "    if algo_name not in evaluation_results:\n",
    "        evaluation_results[algo_name] = {}\n",
    "\n",
    "    for env_name in environments:\n",
    "        # --- Create a combined configuration for the current environment ---\n",
    "        current_config = default_config.copy()\n",
    "        if env_name in env_configs:\n",
    "            current_config.update(env_configs[env_name])\n",
    "        current_config[\"USE_DDQN\"] = use_ddqn_flag\n",
    "\n",
    "        model_path = f\"{algo_name}_{env_name}_policy.pth\"\n",
    "        run_name = f\"{algo_name}_{env_name}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "        # --- Environment Setup ---\n",
    "        is_continuous = False\n",
    "        temp_env = gym.make(env_name)\n",
    "        if isinstance(temp_env.action_space, gym.spaces.Box):\n",
    "            is_continuous = True\n",
    "        temp_env.close()\n",
    "\n",
    "        if is_continuous:\n",
    "            print(f\"--- Detected continuous action space for {env_name}. Applying discretization wrapper. ---\")\n",
    "            env = DiscretizeActionWrapper(gym.make(env_name), n_bins=current_config[\"N_BINS\"])\n",
    "        else:\n",
    "            env = gym.make(env_name)\n",
    "            \n",
    "        n_actions = env.action_space.n\n",
    "        state, _ = env.reset()\n",
    "        n_observations = len(state)\n",
    "        env.reset()\n",
    "\n",
    "        policy_net = DQN(n_observations, n_actions).to(device)\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"--- Found pre-trained model for {algo_name} on {env_name} ---\")\n",
    "            print(f\"Loading model from {model_path} and skipping training.\\n\")\n",
    "            policy_net.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            print(f\"--- Training {algo_name} on {env_name} ---\")\n",
    "            target_net = DQN(n_observations, n_actions).to(device)\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            optimizer = optim.AdamW(policy_net.parameters(), lr=current_config[\"LR\"], amsgrad=True)\n",
    "            memory = ReplayMemory(current_config[\"REPLAY_MEMORY_SIZE\"])\n",
    "            \n",
    "            steps_done = 0  # Reset for each training run\n",
    "            \n",
    "            if USE_WANDB:\n",
    "                if wandb.run is not None: wandb.finish()\n",
    "                wandb.init(project=PROJECT_NAME, name=run_name, config=current_config, reinit=True)\n",
    "\n",
    "            num_episodes = current_config[\"NUM_EPISODES\"]\n",
    "            episode_rewards_train = []\n",
    "            episode_losses = []\n",
    "            \n",
    "            for i_episode in range(num_episodes):\n",
    "                state, info = env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                ep_reward = 0\n",
    "                ep_losses = []\n",
    "                \n",
    "                for t in range(1500):\n",
    "                    action, steps_done = select_action(state, env, policy_net, n_actions, current_config, steps_done)\n",
    "                    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                    ep_reward += reward\n",
    "                    reward = torch.tensor([reward], device=device)\n",
    "                    done = terminated or truncated\n",
    "\n",
    "                    next_state = None if terminated else torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                    memory.push(state, action, next_state, reward)\n",
    "                    state = next_state\n",
    "\n",
    "                    loss = optimize_model(memory, policy_net, target_net, optimizer, current_config)\n",
    "                    if loss is not None:\n",
    "                        ep_losses.append(loss)\n",
    "                    \n",
    "                    # Soft update of target network every step (as per your original design)\n",
    "                    # Alternative: Hard update every N episodes (uncomment below)\n",
    "                    if t % current_config[\"TARGET_UPDATE\"] == 0:\n",
    "                        target_net_state_dict = target_net.state_dict()\n",
    "                        policy_net_state_dict = policy_net.state_dict()\n",
    "                        for key in policy_net_state_dict:\n",
    "                            target_net_state_dict[key] = policy_net_state_dict[key]*current_config[\"TAU\"] + target_net_state_dict[key]*(1-current_config[\"TAU\"])\n",
    "                        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                episode_rewards_train.append(ep_reward)\n",
    "                avg_loss = sum(ep_losses) / len(ep_losses) if ep_losses else 0\n",
    "                episode_losses.append(avg_loss)\n",
    "                \n",
    "                if USE_WANDB and avg_loss > 0:\n",
    "                    wandb.log({\n",
    "                        \"train/reward\": ep_reward, \n",
    "                        \"train/loss\": avg_loss,\n",
    "                        \"train/epsilon\": current_config[\"EPS_END\"] + (current_config[\"EPS_START\"] - current_config[\"EPS_END\"]) * math.exp(-1. * steps_done / current_config[\"EPS_DECAY\"])\n",
    "                    }, step=i_episode)\n",
    "                \n",
    "                if i_episode % 50 == 0:\n",
    "                    print(f\"  Episode {i_episode}/{num_episodes} | Reward: {ep_reward}\")\n",
    "\n",
    "            print(\"--- Training Complete ---\")\n",
    "            torch.save(policy_net.state_dict(), model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "            if USE_WANDB and wandb.run is not None:\n",
    "                wandb.finish()\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        print(f\"--- Evaluating {algo_name} on {env_name} for 100 episodes (with video recording) ---\")\n",
    "        \n",
    "        video_folder = f\"./videos/{algo_name}_{env_name}/\"\n",
    "        os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "        if is_continuous:\n",
    "            eval_env = DiscretizeActionWrapper(gym.make(env_name, render_mode=\"rgb_array\"), n_bins=current_config[\"N_BINS\"])\n",
    "        else:\n",
    "            eval_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        \n",
    "        eval_env = gym.wrappers.RecordVideo(\n",
    "            eval_env, \n",
    "            video_folder,\n",
    "            episode_trigger=lambda x: x % 25 == 0,\n",
    "            name_prefix=f\"{algo_name}-{env_name}\"\n",
    "        )\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_durations = []\n",
    "        for i in range(100):\n",
    "            state, _ = eval_env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            ep_reward = 0\n",
    "            ep_duration = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(state).max(1)[1].view(1, 1)\n",
    "                obs, reward, terminated, truncated, _ = eval_env.step(action.item())\n",
    "                ep_reward += reward\n",
    "                ep_duration += 1\n",
    "                done = terminated or truncated\n",
    "                if not done:\n",
    "                    state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            episode_rewards.append(ep_reward)\n",
    "            episode_durations.append(ep_duration)\n",
    "        \n",
    "        avg_reward = sum(episode_rewards) / 100\n",
    "        evaluation_results[algo_name][env_name] = avg_reward\n",
    "        print(f\"Average Reward over 100 episodes: {avg_reward:.2f}\")\n",
    "        print(f\"Videos saved in: {video_folder}\\n\")\n",
    "        \n",
    "        # Log evaluation data to WandB\n",
    "        if USE_WANDB:\n",
    "            # Initialize wandb for evaluation logging (in case training was skipped)\n",
    "            if wandb.run is None:\n",
    "                wandb.init(project=PROJECT_NAME, name=f\"{run_name}_eval\", config=current_config, reinit=True)\n",
    "            \n",
    "            # Create a wandb.Table\n",
    "            eval_table = wandb.Table(columns=[\"Episode\", \"Reward\", \"Duration\"])\n",
    "            for i in range(100):\n",
    "                eval_table.add_data(i + 1, episode_rewards[i], episode_durations[i])\n",
    "            \n",
    "            # Log the table and summary metrics\n",
    "            wandb.log({\n",
    "                \"evaluation/avg_reward\": avg_reward,\n",
    "                \"evaluation/std_reward\": np.std(episode_rewards),\n",
    "                \"evaluation/max_reward\": max(episode_rewards),\n",
    "                \"evaluation/min_reward\": min(episode_rewards),\n",
    "                \"evaluation/avg_duration\": sum(episode_durations) / 100,\n",
    "                \"evaluation/evaluation_table\": eval_table\n",
    "            })\n",
    "            \n",
    "            if wandb.run is not None:\n",
    "                wandb.finish()\n",
    "\n",
    "        eval_env.close()\n",
    "        env.close()\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\\n--- Overall Evaluation Summary ---\")\n",
    "for algo_name, results in evaluation_results.items():\n",
    "    print(f\"\\n--- {algo_name} Results ---\")\n",
    "    for env_name, avg_reward in results.items():\n",
    "        print(f\"  Environment: {env_name} | Average Reward (100 eps): {avg_reward:.2f}\")\n",
    "print(\"------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
