{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa106ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\abdul\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\abdul\\_netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdelrahmant3\u001b[0m (\u001b[33mabdelrahmant3-cairo-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB login successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from typing import Tuple, List, Deque\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import wandb\n",
    "\n",
    "# --- 1. Introduction: Hyperparameter Tuning Notebook ---\n",
    "\n",
    "# This notebook is designed for running systematic hyperparameter tuning experiments.\n",
    "# It allows you to define multiple experiment configurations and runs them sequentially,\n",
    "# saving the results, trained models, and evaluation videos in separate folders for\n",
    "# easy comparison. This is ideal for use on platforms like Kaggle.\n",
    "\n",
    "# --- Device Detection ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# --- WandB Configuration (Optional) ---\n",
    "# Set USE_WANDB to False if you do not want to log results.\n",
    "USE_WANDB = True\n",
    "PROJECT_NAME = \"RL-Assignment2-Hyperparameter-Tuning\"\n",
    "\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        # This is a placeholder for Kaggle secrets or local key file\n",
    "        # On Kaggle, you would use `from kaggle_secrets import UserSecretsClient`\n",
    "        if os.path.exists(\"key.txt\"):\n",
    "            with open(\"key.txt\", \"r\") as f:\n",
    "                api_key = f.read().strip()\n",
    "            wandb.login(key=api_key)\n",
    "            print(\"WandB login successful.\")\n",
    "        else:\n",
    "            print(\"WandB key file not found. Set USE_WANDB to False or provide a key.\")\n",
    "            USE_WANDB = False\n",
    "    except Exception as e:\n",
    "        print(f\"Could not log in to WandB: {e}\")\n",
    "        USE_WANDB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bcd8e9",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Experiments\n",
    "\n",
    "Instead of a single config, we define a list of `experiments`. Each item in the list is a dictionary containing:\n",
    "- `name`: A unique name for the experiment (used for creating folders).\n",
    "- `config`: A dictionary of hyperparameters for that specific experiment.\n",
    "\n",
    "This structure allows us to test many different configurations in a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4046c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define All Experiment Configurations Here ---\n",
    "\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"Baseline_DQN\",\n",
    "        \"config\": {\n",
    "            \"USE_DDQN\": False,\n",
    "            \"BATCH_SIZE\": 128,\n",
    "            \"GAMMA\": 0.99,\n",
    "            \"EPS_DECAY\": 1000,\n",
    "            \"TAU\": 0.005,\n",
    "            \"LR\": 1e-4,\n",
    "            \"REPLAY_MEMORY_SIZE\": 10000,\n",
    "            \"NET_ARCHITECTURE\": [128, 128],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Baseline_DDQN\",\n",
    "        \"config\": {\n",
    "            \"USE_DDQN\": True,\n",
    "            \"BATCH_SIZE\": 128,\n",
    "            \"GAMMA\": 0.99,\n",
    "            \"EPS_DECAY\": 1000,\n",
    "            \"TAU\": 0.005,\n",
    "            \"LR\": 1e-4,\n",
    "            \"REPLAY_MEMORY_SIZE\": 10000,\n",
    "            \"NET_ARCHITECTURE\": [128, 128],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"High_LR_Fast_Decay\",\n",
    "        \"config\": {\n",
    "            \"USE_DDQN\": True,\n",
    "            \"BATCH_SIZE\": 256,\n",
    "            \"GAMMA\": 0.99,\n",
    "            \"EPS_DECAY\": 500,  # Faster decay\n",
    "            \"TAU\": 0.01, # Faster target update\n",
    "            \"LR\": 5e-4,      # Higher learning rate\n",
    "            \"REPLAY_MEMORY_SIZE\": 10000,\n",
    "            \"NET_ARCHITECTURE\": [128, 128],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Deeper_Network_More_Exploration\",\n",
    "        \"config\": {\n",
    "            \"USE_DDQN\": True,\n",
    "            \"BATCH_SIZE\": 128,\n",
    "            \"GAMMA\": 0.99,\n",
    "            \"EPS_DECAY\": 2000, # Slower decay, more exploration\n",
    "            \"TAU\": 0.005,\n",
    "            \"LR\": 1e-4,\n",
    "            \"REPLAY_MEMORY_SIZE\": 50000,\n",
    "            \"NET_ARCHITECTURE\": [256, 256, 128], # Deeper network\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "# Common parameters that don't change between experiments\n",
    "config = {\n",
    "    \"EPS_START\": 0.9,\n",
    "    \"EPS_END\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabec16f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Core DQN Components\n",
    "\n",
    "This section defines the three fundamental building blocks of the agent:\n",
    "1.  **DQN Model**: A simple feed-forward neural network that estimates Q-values.\n",
    "2.  **Replay Memory**: A buffer that stores past experiences (`state`, `action`, `reward`, `next_state`) so the agent can learn from them in batches. This decorrelates experiences and stabilizes training.\n",
    "3.  **Transition**: A `namedtuple` for conveniently storing a single experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8b6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    # ... (ReplayMemory class remains the same)\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A dynamic DQN model that builds its architecture based on a list of layer sizes.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_observations, n_actions, net_arch):\n",
    "        super(DQN, self).__init__()\n",
    "        layers = []\n",
    "        input_size = n_observations\n",
    "        for output_size in net_arch:\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = output_size\n",
    "        layers.append(nn.Linear(input_size, n_actions))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16c6ad",
   "metadata": {},
   "source": [
    "---\n",
    "### Discretization Wrapper for Continuous Environments\n",
    "\n",
    "Since our DQN agent can only output discrete actions (e.g., action 0, 1, 2), it cannot directly handle environments with continuous action spaces like `Pendulum-v1`.\n",
    "\n",
    "To solve this, we create a **wrapper** class. This class sits \"on top\" of the original environment and modifies its behavior. The `DiscretizeActionWrapper` does the following:\n",
    "1.  It takes the continuous action space (e.g., a range from -2.0 to 2.0 for Pendulum) and converts it into a fixed number of discrete actions (e.g., 5 bins).\n",
    "2.  It tells our agent that there are now 5 possible actions.\n",
    "3.  When our agent picks a discrete action (e.g., action `2`), the wrapper translates it back into the corresponding continuous value (e.g., `0.0`) before passing it to the actual environment.\n",
    "\n",
    "This allows us to use our DQN agent on `Pendulum-v1` without changing the agent's core logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e08c13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizeActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper to discretize a continuous action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_bins):\n",
    "        super().__init__(env)\n",
    "        self.n_bins = n_bins\n",
    "        self.action_space = gym.spaces.Discrete(n_bins)\n",
    "        self.continuous_action_space = env.action_space\n",
    "        \n",
    "        # Create a mapping from discrete actions to continuous values\n",
    "        self.action_map = np.linspace(\n",
    "            self.continuous_action_space.low[0],\n",
    "            self.continuous_action_space.high[0],\n",
    "            n_bins\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the discrete action to a continuous action\n",
    "        continuous_action = np.array([self.action_map[action]], dtype=np.float32)\n",
    "        return self.env.step(continuous_action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39ceb5",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training and Optimization Functions\n",
    "\n",
    "This section contains the core logic for the agent's learning process.\n",
    "\n",
    "- **`select_action`**: Implements an epsilon-greedy policy. With probability `epsilon`, it takes a random action (exploration). Otherwise, it takes the action with the highest predicted Q-value (exploitation).\n",
    "- **`optimize_model`**: This is the heart of the learning algorithm. It samples a batch of experiences from the replay memory and computes the loss. It supports both standard DQN and Double DQN (DDQN) based on the `USE_DDQN` flag in the config.\n",
    "- **`get_env_details`**: A helper to get the action and observation space sizes from an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f57cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state, env, policy_net, n_actions, current_config):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = current_config[\"EPS_END\"] + (current_config[\"EPS_START\"] - current_config[\"EPS_END\"]) * \\\n",
    "        math.exp(-1. * steps_done / current_config[\"EPS_DECAY\"])\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_model(memory, policy_net, target_net, optimizer, current_config):\n",
    "    if len(memory) < current_config[\"BATCH_SIZE\"]:\n",
    "        return None\n",
    "    transitions = memory.sample(current_config[\"BATCH_SIZE\"])\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(current_config[\"BATCH_SIZE\"], device=device)\n",
    "    with torch.no_grad():\n",
    "        if current_config[\"USE_DDQN\"]:\n",
    "            best_actions = policy_net(non_final_next_states).argmax(1).unsqueeze(-1)\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, best_actions).squeeze()\n",
    "        else:\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "            \n",
    "    expected_state_action_values = (next_state_values * current_config[\"GAMMA\"]) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492d5fe",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Main Experiment Runner\n",
    "\n",
    "This is the main execution block. It iterates through every `experiment` defined in Cell 2.\n",
    "\n",
    "For each experiment, it:\n",
    "1.  Creates a unique directory to store the trained models and videos.\n",
    "2.  Merges the specific experiment config with the base config.\n",
    "3.  Loops through each environment (`CartPole`, `Acrobot`, etc.).\n",
    "4.  Trains the agent using the specified hyperparameters.\n",
    "5.  Saves the trained `policy_net` to the experiment's directory.\n",
    "6.  Runs a full evaluation and saves the video recordings to the experiment's directory.\n",
    "7.  Prints a final summary of all results.\n",
    "\n",
    "**This is the only cell you need to run to start the entire tuning process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "115cb4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running Experiment: Baseline_DQN\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Environment: CartPole-v1 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\Assignment 2\\wandb\\run-20251111_224301-i168uawp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Hyperparameter-Tuning/runs/i168uawp' target=\"_blank\">Baseline_DQN_CartPole-v1</a></strong> to <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Hyperparameter-Tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Hyperparameter-Tuning' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Hyperparameter-Tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Hyperparameter-Tuning/runs/i168uawp' target=\"_blank\">https://wandb.ai/abdelrahmant3-cairo-university/RL-Assignment2-Hyperparameter-Tuning/runs/i168uawp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 0/500 | Reward: 20.0\n",
      "  Episode 100/500 | Reward: 12.0\n",
      "  Episode 100/500 | Reward: 12.0\n",
      "  Episode 200/500 | Reward: 132.0\n",
      "  Episode 200/500 | Reward: 132.0\n",
      "  Episode 300/500 | Reward: 110.0\n",
      "  Episode 300/500 | Reward: 110.0\n",
      "  Episode 400/500 | Reward: 288.0\n",
      "  Episode 400/500 | Reward: 288.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m memory.push(state, action, next_state, reward)\n\u001b[32m     72\u001b[39m state = next_state\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m loss = \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m target_net_state_dict = target_net.state_dict()\n\u001b[32m     77\u001b[39m policy_net_state_dict = policy_net.state_dict()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36moptimize_model\u001b[39m\u001b[34m(memory, policy_net, target_net, optimizer, current_config)\u001b[39m\n\u001b[32m     44\u001b[39m loss.backward()\n\u001b[32m     45\u001b[39m torch.nn.utils.clip_grad_value_(policy_net.parameters(), \u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\torch\\optim\\adamw.py:220\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    207\u001b[39m     beta1, beta2 = cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    209\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    210\u001b[39m         group,\n\u001b[32m    211\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m         state_steps,\n\u001b[32m    218\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\torch\\optim\\adamw.py:782\u001b[39m, in \u001b[36madamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    780\u001b[39m     func = _single_tensor_adamw\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Dev\\College\\Reinforcement_Learning\\Assignments\\repo\\.venv\\Lib\\site-packages\\torch\\optim\\adamw.py:604\u001b[39m, in \u001b[36m_multi_tensor_adamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[39m\n\u001b[32m    601\u001b[39m     torch._foreach_maximum_(device_max_exp_avg_sqs, device_exp_avg_sqs)\n\u001b[32m    603\u001b[39m     \u001b[38;5;66;03m# Use the max. for normalizing running avg. of gradient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     exp_avg_sq_sqrt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_max_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    606\u001b[39m     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Main Experiment Runner ---\n",
    "environments = [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\", \"Pendulum-v1\"]\n",
    "overall_results = {}\n",
    "\n",
    "for experiment in experiments:\n",
    "    exp_name = experiment[\"name\"]\n",
    "    exp_config = {**config, **experiment[\"config\"]} # Merge base and experiment-specific configs\n",
    "    algo_name = \"DDQN\" if exp_config[\"USE_DDQN\"] else \"DQN\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\nRunning Experiment: {exp_name}\\n{'='*60}\\n\")\n",
    "    \n",
    "    # Create a dedicated folder for this experiment's outputs\n",
    "    output_dir = f\"./{exp_name}/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    overall_results[exp_name] = {}\n",
    "\n",
    "    for env_name in environments:\n",
    "        print(f\"\\n--- Environment: {env_name} ---\\n\")\n",
    "        \n",
    "        # --- Environment Setup ---\n",
    "        is_continuous = False\n",
    "        temp_env = gym.make(env_name)\n",
    "        if isinstance(temp_env.action_space, gym.spaces.Box):\n",
    "            is_continuous = True\n",
    "        temp_env.close()\n",
    "\n",
    "        if is_continuous:\n",
    "            env = DiscretizeActionWrapper(gym.make(env_name), n_bins=11)\n",
    "        else:\n",
    "            env = gym.make(env_name)\n",
    "            \n",
    "        n_actions = env.action_space.n\n",
    "        state, _ = env.reset()\n",
    "        n_observations = len(state)\n",
    "        env.reset()\n",
    "\n",
    "        # --- Model Initialization ---\n",
    "        policy_net = DQN(n_observations, n_actions, exp_config[\"NET_ARCHITECTURE\"]).to(device)\n",
    "        target_net = DQN(n_observations, n_actions, exp_config[\"NET_ARCHITECTURE\"]).to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        optimizer = optim.AdamW(policy_net.parameters(), lr=exp_config[\"LR\"], amsgrad=True)\n",
    "        memory = ReplayMemory(exp_config[\"REPLAY_MEMORY_SIZE\"])\n",
    "        \n",
    "        steps_done = 0\n",
    "        \n",
    "        # --- WandB Setup ---\n",
    "        run_name = f\"{exp_name}_{env_name}\"\n",
    "        if USE_WANDB:\n",
    "            if wandb.run is not None: wandb.finish()\n",
    "            wandb.init(project=PROJECT_NAME, name=run_name, config=exp_config, reinit=True)\n",
    "\n",
    "        # --- Training Loop ---\n",
    "        num_episodes = 2000 if env_name == \"MountainCar-v0\" else 1000\n",
    "        for i_episode in range(num_episodes):\n",
    "            state, info = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            ep_reward = 0\n",
    "            \n",
    "            for t in range(1000):\n",
    "                action = select_action(state, env, policy_net, n_actions, exp_config)\n",
    "                observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                ep_reward += reward\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if terminated: next_state = None\n",
    "                else: next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                memory.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "\n",
    "                loss = optimize_model(memory, policy_net, target_net, optimizer, exp_config)\n",
    "                \n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*exp_config[\"TAU\"] + target_net_state_dict[key]*(1-exp_config[\"TAU\"])\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done: break\n",
    "            \n",
    "            if USE_WANDB and loss is not None: wandb.log({\"reward\": ep_reward, \"loss\": loss}, step=i_episode)\n",
    "            if i_episode % 100 == 0: print(f\"  Episode {i_episode}/{num_episodes} | Reward: {ep_reward}\")\n",
    "\n",
    "        print(\"--- Training Complete ---\")\n",
    "        model_path = os.path.join(output_dir, f\"{algo_name}_{env_name}_policy.pth\")\n",
    "        torch.save(policy_net.state_dict(), model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "        if USE_WANDB: wandb.finish()\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        print(f\"--- Evaluating for 100 episodes (with video recording) ---\")\n",
    "        video_folder = os.path.join(output_dir, \"videos\", f\"{algo_name}_{env_name}\")\n",
    "        os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "        if is_continuous:\n",
    "            eval_env = DiscretizeActionWrapper(gym.make(env_name, render_mode=\"rgb_array\"), n_bins=11)\n",
    "        else:\n",
    "            eval_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        \n",
    "        eval_env = gym.wrappers.RecordVideo(eval_env, video_folder, episode_trigger=lambda x: x % 20 == 0, name_prefix=f\"{exp_name}-{env_name}\")\n",
    "\n",
    "        total_eval_reward = 0\n",
    "        for i in range(100):\n",
    "            state, _ = eval_env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            ep_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(state).max(1)[1].view(1, 1)\n",
    "                obs, reward, terminated, truncated, _ = eval_env.step(action.item())\n",
    "                ep_reward += reward\n",
    "                done = terminated or truncated\n",
    "                if not done: state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            total_eval_reward += ep_reward\n",
    "        \n",
    "        avg_reward = total_eval_reward / 100\n",
    "        overall_results[exp_name][env_name] = avg_reward\n",
    "        print(f\"Average Reward over 100 episodes: {avg_reward:.2f}\")\n",
    "        print(f\"Videos saved in: {video_folder}\\n\")\n",
    "        \n",
    "        eval_env.close()\n",
    "        env.close()\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\\n--- Overall Evaluation Summary ---\")\n",
    "for exp_name, results in overall_results.items():\n",
    "    print(f\"\\n--- Experiment: {exp_name} ---\")\n",
    "    for env_name, avg_reward in results.items():\n",
    "        print(f\"  {env_name}: Average Reward (100 eps) = {avg_reward:.2f}\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a18449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
